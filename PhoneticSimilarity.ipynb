{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PhoneticSimilarity.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNfrqK2Pwk7cz40FSHWZcMz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThreeMachineExpression/contrapuntalPoetry/blob/main/PhoneticSimilarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPCyH2eLsbbF"
      },
      "source": [
        "\r\n",
        "Parts of this notebook are based on Max Woolf's aitextgen notebook, as updated by Allison Parrish.\r\n",
        "\r\n",
        "Also makes use of Kyle Gorman's syllabification library, syllabify.\r\n",
        "\r\n",
        "Initial implementation looked for slant rhyme / slant alliteration by grabbing all of the consonants from the boundaries between vowels and summing their features, but that leads to too-mushy results. The syllabification approach misses assonance that crosses syllable boundaries - a todo is to figure out how to bring some of that back.\r\n",
        "\r\n",
        "Todo: correctly handle single letters (as in e.g.). It sounds them out correctly, but adds two STOPs between each for the periods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxzLNBUjsxSv"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYABCBZZs2ie",
        "outputId": "ecf9caba-f01c-4a4f-fe51-4a176a6517c5"
      },
      "source": [
        "# Freeze versions of dependencies for now\r\n",
        "!pip install tensorflow==1.15.0 keras==2.2.5 \"h5py<3.0.0\"\r\n",
        "!pip3 install pytorch-lightning==0.7.6\r\n",
        "!pip3 install transformers==2.9.1\r\n",
        "!pip3 install fire==0.3.0\r\n",
        "\r\n",
        "!pip install -q aitextgen==0.2.3\r\n",
        "\r\n",
        "from aitextgen import aitextgen\r\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive\r\n",
        "\r\n",
        "mount_gdrive()\r\n",
        "\r\n",
        "!pip install annoy\r\n",
        "!pip install pronouncing\r\n",
        "!pip install pincelate\r\n",
        "import pronouncing\r\n",
        "import pincelate\r\n",
        "pin = pincelate.Pincelate()\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "import random\r\n",
        "import textwrap\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import logging\r\n",
        "logging.basicConfig(\r\n",
        "        format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\r\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n",
        "        level=logging.INFO\r\n",
        "    )\r\n",
        "\r\n",
        "kPhoneticSimilarityVectorsRepo = \"https://github.com/aparrish/phonetic-similarity-vectors\"\r\n",
        "\r\n",
        "!git clone {kPhoneticSimilarityVectorsRepo}\r\n",
        "\r\n",
        "!python -m spacy download en_core_web_md\r\n",
        "\r\n",
        "%cd phonetic-similarity-vectors\r\n",
        "from featurephone import phone_feature_map as pfm\r\n",
        "%cd ..\r\n",
        "\r\n",
        "kSyllabifyRepo = \"https://github.com/threemachineexpression/syllabify\"\r\n",
        "!git clone {kSyllabifyRepo}\r\n",
        "\r\n",
        "%cd syllabify\r\n",
        "import syllabify\r\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 40kB/s \n",
            "\u001b[?25hCollecting keras==2.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/ba/2d058dcf1b85b9c212cc58264c98a4a7dd92c989b798823cc5690d062bb2/Keras-2.2.5-py2.py3-none-any.whl (336kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py<3.0.0 in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 43.8MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.32.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.10.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.12.4)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (51.3.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=5b90775a353c377488bc24ebc937682c397dc94ff0108d02baf979bf560c61c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, gast, keras-applications, tensorflow-estimator, tensorflow, keras\n",
            "  Found existing installation: tensorboard 2.4.0\n",
            "    Uninstalling tensorboard-2.4.0:\n",
            "      Successfully uninstalled tensorboard-2.4.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.0\n",
            "    Uninstalling tensorflow-2.4.0:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZkOeQ8V4zhX"
      },
      "source": [
        "from_folder = \"aitextgenDreamFineTuning\"\r\n",
        "\r\n",
        "for file in [\"pytorch_model.bin\", \"config.json\"]:\r\n",
        "  if from_folder:\r\n",
        "    copy_file_from_gdrive(file, from_folder)\r\n",
        "  else:\r\n",
        "    copy_file_from_gdrive(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9afxhkk46WG"
      },
      "source": [
        "ai = aitextgen(model=\"pytorch_model.bin\", config=\"config.json\", to_gpu=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXb_7Rin5hr-"
      },
      "source": [
        "# Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DdUOTYE5tad",
        "outputId": "ef7ed54f-7294-45a9-ad3d-2675d214b53d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jan 24 02:49:38 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    40W / 300W |  16087MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H4_V452Dy5u",
        "outputId": "53cbae9b-445f-4de4-dc82-d391c2593a70"
      },
      "source": [
        "ai.generate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "You've made my night so much easier.\n",
            "Oh, I'm so sorry.\n",
            "I'm so sorry.\n",
            "I'm so sorry.\n",
            "I know you didn't mean it, but I know you didn't.\n",
            "I was just so sorry I couldn't get out from under you.\n",
            "And I could be kicked out of Eastman for it, but really I could be kicked out for anything, I'm already sneezing the wrong way, I'm starving, and having a really bad case of scabies, and I wake up the next morning thinking I've been impregnated, and it wouldn't have happened if I hadn't had the thought that what you were thinking.\n",
            "So I'm trying really hard to think of what I'm going to say to get out of this, to make sure no one sees it, to make sure that no one sees my weaknesses, to make sure that the people who love me don't hurt me when I say it, to make sure that the damage isn't irreparable.\n",
            "I'm trying really hard to pretend that I don't know anything about this kid, that I know how special this kid is and how talented this kid is, that I don't have to see his face in real\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftsVytZcEGLj"
      },
      "source": [
        "# Phonetic Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVnfSUgmEqNd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "3e5741e3-9d62-477c-f2e9-5a237a986ec5"
      },
      "source": [
        "# Strategy:\r\n",
        "# Split text into syllable buckets\r\n",
        "# In each bucket, include the vowel sound, stress, and all of the bordering phonemes\r\n",
        "#  (so a consonant between syllables goes in both buckets)\r\n",
        "# Measure the distance between two buckets as \r\n",
        "#   a * (manhattan distance of consonant features)\r\n",
        "# + b * (manhattan distance of vowel features)\r\n",
        "# + c * (stress distance)\r\n",
        "# (with a, b, c tuned as desired to emphasize different similarities)\r\n",
        "#\r\n",
        "# Pauses get a bucket of their own.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class Syllable:\r\n",
        "  # initialization takes a list of 3 lists of strings:\r\n",
        "  # the phones for onset, nucleus, and coda\r\n",
        "  # (as per the output of syllabify).\r\n",
        "  #\r\n",
        "  # vowel - vowel phone (w/o stress indicator) or STOP\r\n",
        "  # stress - 0 for STOP, 1 for unstressed, 2 2ndary stress, 3 primary stress\r\n",
        "  # (note this is different from the arpabet numbers)\r\n",
        "  # ofeat - Counter containing a count of the consonant features in the onset\r\n",
        "  # cfeat - Counter containing a count of the consonant features in the coda\r\n",
        "  # vfeat - Counter containing a count of the vowel features\r\n",
        "  def __init__(self, syllableList):\r\n",
        "    self.vowel = syllableList[1][0][0:-1]\r\n",
        "\r\n",
        "    # convert arpabet stress to a representation capable of similarity math\r\n",
        "    stressConversion = {\r\n",
        "        '0': 1,\r\n",
        "        '1': 3,\r\n",
        "        '2': 2\r\n",
        "    }\r\n",
        "\r\n",
        "    if self.vowel == 'STOP':\r\n",
        "      self.stress = 0\r\n",
        "    else:\r\n",
        "      self.stress = stressConversion[syllableList[1][0][-1]]\r\n",
        "      \r\n",
        "    self.ofeat = Counter()\r\n",
        "    self.cfeat = Counter()\r\n",
        "    self.vfeat = Counter()\r\n",
        "\r\n",
        "    for feature in pfm[self.vowel]:\r\n",
        "      self.vfeat[feature] += 1\r\n",
        "    \r\n",
        "    for phone in syllableList[0]:\r\n",
        "      for feature in pfm[phone]:\r\n",
        "        self.ofeat[feature] += 1\r\n",
        "    \r\n",
        "    for phone in syllableList[2]:\r\n",
        "      for feature in pfm[phone]:\r\n",
        "        self.cfeat[feature] += 1\r\n",
        "\r\n",
        "pfm['STOP'] = ()\r\n",
        "stopSyllable = Syllable(([],['STOP0'],[]))\r\n",
        "\r\n",
        "vowels = ('AO','AA','IY','UW','EH','IH','UH','AH','AE','EY','AY','OW','AW','OY',\r\n",
        "          'ER','STOP')\r\n",
        "consonantFeatures = ('alv','apr','asp','blb','dnt','frc','glt','lat','lbd','lbv','nas','pal','pla','stp','vcd','vel','vls')\r\n",
        "vowelFeatures = ('bck','cnt','fnt','hgh','lmd','low','mid','rnd','rzd','smh','umd','unr','vwl')\r\n",
        "doublestops = ['.','?','!',':','--']\r\n",
        "singlestops = [',',';','- ']\r\n",
        "\r\n",
        "# When deciding how close two syllables are, these constants indicate how much\r\n",
        "# weight to give vowel features, stress, and consonant features\r\n",
        "kDistanceWeightVowels = 3.0\r\n",
        "kDistanceWeightStress = 3.0\r\n",
        "kDistanceWeightOnset = 2.0\r\n",
        "kDistanceWeightCoda = 5.0\r\n",
        "\r\n",
        "def phones_for_word_fb(word):\r\n",
        "  \"\"\"Phones for word - either 1st entry in the CMU pronouncing dictionary\r\n",
        "  or fallback to Pincelate sound-out if it's not in the dictionary.\r\n",
        "\r\n",
        "  Takes the first pronounciation in the dictionary if there are multiple.\r\n",
        "\r\n",
        "  Returns a list of phones.\r\n",
        "\r\n",
        "  word - lowercase word, no spaces or punctuation\r\n",
        "  \"\"\"\r\n",
        "  try:\r\n",
        "    return pronouncing.phones_for_word(word)[0].split()\r\n",
        "  except IndexError:\r\n",
        "    r = pin.soundout(word)\r\n",
        "    # Throw in an extra 'EH0' if there are no vowels in the mix already\r\n",
        "    for v, p in [(v,p) for v in vowels for p in r]:\r\n",
        "      if v in p:\r\n",
        "        return r\r\n",
        "    r.append('EH0')\r\n",
        "    return r\r\n",
        "\r\n",
        "def syllablesFromText(string):\r\n",
        "  \"\"\"Turns text into a list of Syllable objects.\r\n",
        "\r\n",
        "  One stop for ,; two stops for .:?! or double dash.\r\n",
        "  Turns a hyphen between letters (as in \"use-case\") into a space,\r\n",
        "   but in \"a phrase - like this one\" replaces it with a STOP\r\n",
        "\r\n",
        "  Discards all other punctuation and non-alpha characters (including numbers).\r\n",
        "\r\n",
        "  string -- text to convert to phones\"\"\"\r\n",
        "\r\n",
        "  lidx = ridx = 0\r\n",
        "  \r\n",
        "  syllables = []\r\n",
        "\r\n",
        "  while (lidx < len(string)):\r\n",
        "    if string[lidx].isalpha():\r\n",
        "      while ((ridx < len(string)) and (string[ridx].isalpha())):\r\n",
        "        ridx += 1\r\n",
        "      for s in syllabify.syllabify(phones_for_word_fb(string[lidx:ridx].lower())):\r\n",
        "        syllables.append(Syllable(s))\r\n",
        "      lidx = ridx\r\n",
        "    else:\r\n",
        "      while ((ridx < len(string)) and (not string[ridx].isalpha())):\r\n",
        "        ridx += 1\r\n",
        "      doublestop = False\r\n",
        "      if any(c in string[lidx:ridx] for c in doublestops):\r\n",
        "        syllables.extend([stopSyllable, stopSyllable])\r\n",
        "      else:\r\n",
        "        if any(c in string[lidx:ridx] for c in singlestops):\r\n",
        "          syllables.append(stopSyllable)\r\n",
        "      lidx = ridx\r\n",
        "\r\n",
        "  return syllables\r\n",
        "\r\n",
        "\r\n",
        "                \r\n",
        "def distance(a : Syllable, b : Syllable):\r\n",
        "  \"\"\"A distance metric between two Syllable objects.\r\n",
        "  Adjusted by the constant weights in the first cell,\r\n",
        "  applied to Manhattan distance on features.\r\n",
        "  \"\"\" \r\n",
        "  dist = abs(a.stress - b.stress) * kDistanceWeightStress\r\n",
        "\r\n",
        "  for f in consonantFeatures:\r\n",
        "    dist += abs(a.ofeat[f] - b.ofeat[f]) * kDistanceWeightOnset\r\n",
        "    dist += abs(a.cfeat[f] - b.cfeat[f]) * kDistanceWeightCoda\r\n",
        "  \r\n",
        "  for f in vowelFeatures:\r\n",
        "    dist += abs(a.vfeat[f] - b.vfeat[f]) * kDistanceWeightVowels\r\n",
        "\r\n",
        "  return dist\r\n",
        "\r\n",
        "def isVowel(phone):\r\n",
        "  return phone[0:-1] in vowels\r\n",
        "\r\n",
        "def canonFit(oldSyllables, newText, offset):\r\n",
        "  \"\"\"Given a syllable list and a batch of new text (as a string),\r\n",
        "  returns the average syllable distance of newly added syllables\r\n",
        "   to syllables *offset* syllables behind.\r\n",
        "  \r\n",
        "  Lower is rhymier.\r\n",
        "  \r\n",
        "  If newText doesn't turn up any new syllables, return a stupidly big number.\r\n",
        "\r\n",
        "  Warning: GPT-2 continuations can add partial words or just whitespace or\r\n",
        "  punctuation. This method makes no effort to re-syllabize the end of\r\n",
        "  oldSyllables or use that information to pronounce the beginning of newText.\r\n",
        "  Prepare data along word boundaries before passing to this method.\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  newSyllables = syllablesFromText(newText)\r\n",
        "\r\n",
        "  if len(newSyllables) == 0:\r\n",
        "    return float(\"inf\")\r\n",
        "  \r\n",
        "  existingLength = len(oldSyllables)\r\n",
        "  totalDist = i = averageCounter = 0\r\n",
        "\r\n",
        "  for s in newSyllables:\r\n",
        "    idx = existingLength - offset + i\r\n",
        "    # don't look back past the beginning of the text\r\n",
        "    if idx >= 0:    \r\n",
        "      if idx < existingLength:\r\n",
        "        t = oldSyllables[idx]\r\n",
        "      else:\r\n",
        "        t = newSyllables[idx - existingLength]\r\n",
        "      # value stressed rhymes more\r\n",
        "      multiplier = max(1, s.stress, t.stress)\r\n",
        "      totalDist += distance(s, t) * multiplier\r\n",
        "      averageCounter += multiplier\r\n",
        "    i += 1\r\n",
        "  \r\n",
        "  if averageCounter > 0:\r\n",
        "    return totalDist/averageCounter\r\n",
        "  else:\r\n",
        "    # haven't hit the canon start point, or only added STOPs that match up with STOPs;\r\n",
        "    # accept this continuation and keep going\r\n",
        "    # (theoretical risk of getting stuck in STOPland - we'll see if that's a real problem)\r\n",
        "    return 0\r\n",
        "\r\n",
        "def isascii(string):\r\n",
        "  # Apparently this is a fast way of checking because the conversion\r\n",
        "  # is implemented in C\r\n",
        "  try:\r\n",
        "    string.encode('ascii')\r\n",
        "  except UnicodeEncodeError:\r\n",
        "    return False\r\n",
        "  else:\r\n",
        "    return True"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f3a88451de08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfeat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mpfm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'STOP'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mstopSyllable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSyllable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'STOP0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pfm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcjQ0BKKELvx"
      },
      "source": [
        "class Continuations:\r\n",
        "  def __init__(self, alts, prompt, max_length, temperature, top_p):\r\n",
        "    self.prompt = prompt\r\n",
        "    self.max_length = max_length\r\n",
        "    self.temperature = temperature\r\n",
        "    self.top_p = top_p\r\n",
        "\r\n",
        "    self.leftToGenerate = alts\r\n",
        "    self.counter = 0\r\n",
        "    self.continuationsToTry = 256\r\n",
        "    self.cacheIter = iter([])\r\n",
        "\r\n",
        "  def __iter__(self):\r\n",
        "    return self\r\n",
        "\r\n",
        "  def __next__(self):\r\n",
        "    try:\r\n",
        "      return self.cacheIter.__next__()\r\n",
        "    except StopIteration:\r\n",
        "      if self.leftToGenerate == 0:\r\n",
        "        raise(StopIteration)\r\n",
        "        \r\n",
        "      while self.continuationsToTry > 1:\r\n",
        "        try:\r\n",
        "          n = min(self.leftToGenerate, self.continuationsToTry)\r\n",
        "          self.cacheIter = iter(ai.generate(n=n,\r\n",
        "                                       prompt=self.prompt,\r\n",
        "                                       max_length=self.max_length,\r\n",
        "                                       temperature=self.temperature,\r\n",
        "                                       top_p=self.top_p,\r\n",
        "                                       return_as_list=True))\r\n",
        "          self.leftToGenerate = max(0, self.leftToGenerate - n)\r\n",
        "          return self.cacheIter.__next__()\r\n",
        "        except RuntimeError as e:\r\n",
        "          print(\"Runtime Error, reducing batch size: \" + str(e))\r\n",
        "          self.continuationsToTry = self.continuationsToTry//2\r\n",
        "      \r\n",
        "      self.cacheIter = iter(ai.generate(n=self.continuationsToTry,\r\n",
        "                                       prompt=self.prompt,\r\n",
        "                                       max_length=self.max_length,\r\n",
        "                                       temperature=self.temperature,\r\n",
        "                                       top_p=self.top_p,\r\n",
        "                                       return_as_list=True))\r\n",
        "      return self.cacheIter.__next__()\r\n",
        "\r\n",
        "\r\n",
        "def generateCanon(alts = 100, tokensPerIncrement = 5, length = 100,\r\n",
        "                  prompt = \"If only I could be a little more\",\r\n",
        "                  temperature = 1.2, top_p = 0.9, offset = 10):\r\n",
        "  \"\"\"\r\n",
        "  alts - number of samples of text to generate to test for rhymes\r\n",
        "  tokensPerIncrement - size of each incremental sample\r\n",
        "  length - total tokens (including the prompt)\r\n",
        "  prompt - start of canon\r\n",
        "  temperature, top_p - passed along to GPT-2\r\n",
        "  offset - how many syllables the 2nd part of the canon is delayed\r\n",
        "  \"\"\"\r\n",
        "  print(prompt)\r\n",
        "  canonSoFar = prompt\r\n",
        "  seed = 0\r\n",
        "  startingTokens = len(ai.tokenizer.tokenize(text=prompt))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  for i in range(startingTokens, length, tokensPerIncrement):\r\n",
        "    # Tricky thing - GPT-2's tokens are sometimes non-words and partial words.\r\n",
        "    # \"don\" might end a previous pass and be expanded to \"don't\" in a continuation.\r\n",
        "    # Or a continuation could add only line breaks and punctuation.    \r\n",
        "    # So we split canonSoFar up to the last non-alpha character.\r\n",
        "    # We'll attach the rest of canonSoFar to the newly generated text.\r\n",
        "    lastNonAlphaInCanonSoFar = len(canonSoFar) - 1\r\n",
        "    while lastNonAlphaInCanonSoFar > 0:\r\n",
        "      if not canonSoFar[lastNonAlphaInCanonSoFar].isalpha():\r\n",
        "        break\r\n",
        "      lastNonAlphaInCanonSoFar = lastNonAlphaInCanonSoFar - 1\r\n",
        "    \r\n",
        "    solidCanon = canonSoFar[0:lastNonAlphaInCanonSoFar]\r\n",
        "    tentativeCanon = canonSoFar[lastNonAlphaInCanonSoFar:]\r\n",
        "    \r\n",
        "    solidCanonSyllables = syllablesFromText(solidCanon)  \r\n",
        "    \r\n",
        "    bestTrial = canonSoFar\r\n",
        "    bestDist = float(\"inf\")\r\n",
        "\r\n",
        "    continuations = Continuations(alts, canonSoFar, i, temperature,\r\n",
        "                                  top_p)\r\n",
        "    for trial in continuations:\r\n",
        "      newText = tentativeCanon + trial[len(canonSoFar):]\r\n",
        "      if (isascii(newText)  # only test generated text that our pronounciation lookups can handle\r\n",
        "          and not any(c.isdigit() for c in newText)):  # discard text with numbers \r\n",
        "        trialFit = canonFit(solidCanonSyllables, newText, offset)\r\n",
        "        if trialFit < bestDist and trialFit > 0: # discourage exact loops\r\n",
        "          bestDist = trialFit\r\n",
        "          bestTrial = trial\r\n",
        "    \r\n",
        "    canonSoFar = bestTrial\r\n",
        "    print()\r\n",
        "    print(canonSoFar)\r\n",
        "\r\n",
        "  print()\r\n",
        "  print(\"FINAL CANON:\")\r\n",
        "  print(canonSoFar)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VMJq4r23Iolk",
        "outputId": "17b13e61-c780-425e-b08d-ad840dc8f0eb"
      },
      "source": [
        "generateCanon(alts = 250, tokensPerIncrement=2, length=200, prompt=\"A transition matrix\", offset = 10, temperature = .5, top_p = 0.9)\r\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A transition matrix\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 15.90 GiB total capacity; 14.70 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Generating 122 continuations\n",
            "\n",
            "A transition matrix\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 15.90 GiB total capacity; 14.70 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Generating 122 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 336.00 MiB (GPU 0; 15.90 GiB total capacity; 14.57 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Generating 64 continuations\n",
            "Generating 64 continuations\n",
            "Generating 58 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 15.90 GiB total capacity; 14.61 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 0; 15.90 GiB total capacity; 14.53 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Generating 64 continuations\n",
            "Generating 64 continuations\n",
            "Generating 58 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 528.00 MiB (GPU 0; 15.90 GiB total capacity; 14.65 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 270.00 MiB (GPU 0; 15.90 GiB total capacity; 14.55 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Generating 64 continuations\n",
            "Generating 64 continuations\n",
            "Generating 58 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 320.00 MiB (GPU 0; 15.90 GiB total capacity; 14.57 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 26 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 370.00 MiB (GPU 0; 15.90 GiB total capacity; 14.59 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 186.00 MiB (GPU 0; 15.90 GiB total capacity; 14.52 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 26 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 15.90 GiB total capacity; 14.70 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 418.00 MiB (GPU 0; 15.90 GiB total capacity; 14.60 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 210.00 MiB (GPU 0; 15.90 GiB total capacity; 14.53 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 26 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 15.90 GiB total capacity; 14.71 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 468.00 MiB (GPU 0; 15.90 GiB total capacity; 14.62 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 234.00 MiB (GPU 0; 15.90 GiB total capacity; 14.54 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 26 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 15.90 GiB total capacity; 14.73 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 516.00 MiB (GPU 0; 15.90 GiB total capacity; 14.64 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 0; 15.90 GiB total capacity; 14.54 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 26 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 566.00 MiB (GPU 0; 15.90 GiB total capacity; 14.66 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 284.00 MiB (GPU 0; 15.90 GiB total capacity; 14.55 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 26 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 15.90 GiB total capacity; 14.72 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 15.90 GiB total capacity; 14.75 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 308.00 MiB (GPU 0; 15.90 GiB total capacity; 14.56 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 154.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 15.90 GiB total capacity; 14.66 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 332.00 MiB (GPU 0; 15.90 GiB total capacity; 14.57 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 15.90 GiB total capacity; 14.68 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 356.00 MiB (GPU 0; 15.90 GiB total capacity; 14.58 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 178.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 15.90 GiB total capacity; 14.69 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 15.90 GiB total capacity; 14.80 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 382.00 MiB (GPU 0; 15.90 GiB total capacity; 14.59 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 15.90 GiB total capacity; 14.52 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform,\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 15.90 GiB total capacity; 14.61 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 15.90 GiB total capacity; 14.70 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 406.00 MiB (GPU 0; 15.90 GiB total capacity; 14.60 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 204.00 MiB (GPU 0; 15.90 GiB total capacity; 14.52 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 0; 15.90 GiB total capacity; 14.62 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 0; 15.90 GiB total capacity; 14.77 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 430.00 MiB (GPU 0; 15.90 GiB total capacity; 14.61 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 15.90 GiB total capacity; 14.53 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 0; 15.90 GiB total capacity; 14.63 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 15.90 GiB total capacity; 14.71 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 454.00 MiB (GPU 0; 15.90 GiB total capacity; 14.62 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 15.90 GiB total capacity; 14.53 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 0; 15.90 GiB total capacity; 14.65 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 15.90 GiB total capacity; 14.72 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 480.00 MiB (GPU 0; 15.90 GiB total capacity; 14.63 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 0; 15.90 GiB total capacity; 14.54 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 0; 15.90 GiB total capacity; 14.65 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 15.90 GiB total capacity; 14.73 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 504.00 MiB (GPU 0; 15.90 GiB total capacity; 14.64 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 252.00 MiB (GPU 0; 15.90 GiB total capacity; 14.54 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 0; 15.90 GiB total capacity; 14.66 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 15.90 GiB total capacity; 14.75 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 528.00 MiB (GPU 0; 15.90 GiB total capacity; 14.65 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 264.00 MiB (GPU 0; 15.90 GiB total capacity; 14.55 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 554.00 MiB (GPU 0; 15.90 GiB total capacity; 14.66 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 15.90 GiB total capacity; 14.55 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 15.90 GiB total capacity; 14.78 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 15.90 GiB total capacity; 14.78 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 15.90 GiB total capacity; 14.79 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 290.00 MiB (GPU 0; 15.90 GiB total capacity; 14.56 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 146.00 MiB (GPU 0; 15.90 GiB total capacity; 14.50 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The fundamental electricity\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 15.90 GiB total capacity; 14.72 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 15.90 GiB total capacity; 14.74 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 15.90 GiB total capacity; 14.56 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 152.00 MiB (GPU 0; 15.90 GiB total capacity; 14.50 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The fundamental electricity of me\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 15.90 GiB total capacity; 14.74 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 15.90 GiB total capacity; 14.65 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 314.00 MiB (GPU 0; 15.90 GiB total capacity; 14.57 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 158.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The fundamental electricity of me is being\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 15.90 GiB total capacity; 14.68 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 15.90 GiB total capacity; 14.66 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 15.90 GiB total capacity; 14.75 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 15.90 GiB total capacity; 14.57 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The fundamental electricity of me is being pulled apart\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 15.90 GiB total capacity; 14.68 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 15.90 GiB total capacity; 14.67 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 15.90 GiB total capacity; 14.78 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 338.00 MiB (GPU 0; 15.90 GiB total capacity; 14.58 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The fundamental electricity of me is being pulled apart, and\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 15.90 GiB total capacity; 14.69 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 15.90 GiB total capacity; 14.68 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 15.90 GiB total capacity; 14.77 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 350.00 MiB (GPU 0; 15.90 GiB total capacity; 14.58 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 176.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The fundamental electricity of me is being pulled apart, and it is\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 0; 15.90 GiB total capacity; 14.70 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 15.90 GiB total capacity; 14.69 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 15.90 GiB total capacity; 14.78 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 362.00 MiB (GPU 0; 15.90 GiB total capacity; 14.58 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 0; 15.90 GiB total capacity; 14.52 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The fundamental electricity of me is being pulled apart, and it is not being\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 15.90 GiB total capacity; 14.71 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 15.90 GiB total capacity; 14.69 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 15.90 GiB total capacity; 14.77 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 15.90 GiB total capacity; 14.59 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 0; 15.90 GiB total capacity; 14.52 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The fundamental electricity of me is being pulled apart, and it is not being\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 15.90 GiB total capacity; 14.71 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 15.90 GiB total capacity; 14.69 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 15.90 GiB total capacity; 14.77 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 15.90 GiB total capacity; 14.59 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 0; 15.90 GiB total capacity; 14.52 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.90 GiB total capacity; 14.47 GiB already allocated; 1.88 MiB free; 14.94 GiB reserved in total by PyTorch)\n",
            "Generating 2 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.90 GiB total capacity; 14.47 GiB already allocated; 1.88 MiB free; 14.94 GiB reserved in total by PyTorch)\n",
            "Generating 2 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.90 GiB total capacity; 14.47 GiB already allocated; 1.88 MiB free; 14.94 GiB reserved in total by PyTorch)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-cf8e5619f2dc>\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcacheIter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-28245f32ce29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerateCanon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokensPerIncrement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"A transition matrix\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-cf8e5619f2dc>\u001b[0m in \u001b[0;36mgenerateCanon\u001b[0;34m(alts, tokensPerIncrement, length, prompt, temperature, top_p, offset)\u001b[0m\n\u001b[1;32m     86\u001b[0m     continuations = Continuations(alts, canonSoFar, i, temperature,\n\u001b[1;32m     87\u001b[0m                                   top_p)\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontinuations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0mnewText\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtentativeCanon\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanonSoFar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       if (isascii(newText)  # only test generated text that our pronounciation lookups can handle\n",
            "\u001b[0;32m<ipython-input-29-cf8e5619f2dc>\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m                                        \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                                        \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                                        return_as_list=True))\n\u001b[0m\u001b[1;32m     45\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcacheIter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/aitextgen/aitextgen.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, n, prompt, max_length, temperature, do_sample, return_as_list, seed, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         )\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_specific_kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m             )\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_generate_no_beam_search\u001b[0;34m(self, input_ids, cur_len, max_length, min_length, do_sample, temperature, top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, decoder_start_token_id, batch_size, encoder_outputs, attention_mask, use_cache, model_specific_kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m             )\n\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache)\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m         )\n\u001b[1;32m    617\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache)\u001b[0m\n\u001b[1;32m    497\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m             )\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, use_cache)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_attn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# output_attn: a, present, (attentions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, use_cache)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresid_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1710\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1713\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXmPunR8-E2w"
      },
      "source": [
        "with new syllabification, temperature 5, top_p .2, tokensPerIncrement = 5, offset 5, params 4/5/2/3 (vowels/stress/onset/coda)\r\n",
        "\r\n",
        "```\r\n",
        "If only I could be in a meeting coordinating this, explaining things.\r\n",
        "Getting these people to believe I can't deliver on time has never made any difference, even in the event that they're irate and cry when I'm explaining why we didn't, I haven't, I haven't, it never was; it never will, it never will; if I were building this case, building this case, building a case about me and [1000Plateaus] I probably would have had a clearer shot at what people thought I was doing than I was.\r\n",
        "I am as bad as I think myself as I can be when it happens to me - I can't be in an arty tent in an alleyway in an arty prison, hardly eating, hardly paying my tab, staring at strangers ferried here from the slobbery of the north, a dozen screaming asylum-climbers and clinging desperately to their property - I have plenty of time, money\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WH8dvGm2miz"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "If  only I  could be in that  boat.       That boat.\r\n",
        "            If    only  I     could be in that boat.\r\n",
        "\r\n",
        "I    don't know why.       Cousinwog, cousin of  Brassy and\r\n",
        "That boat.         I don't know why.  Cousinwog,     cousin \r\n",
        "\r\n",
        "Red from    Cousinwog  and   Mighty from   Cousinwog  \r\n",
        "of  Brassy     and Red from  Cousinwog    and Mighty from\r\n",
        "\r\n",
        "City  and  Plateaus  aren't  together        anymore.   They're\r\n",
        "Cousinwog  City      and     Plateaus aren't together anymore. \r\n",
        "\r\n",
        "having a breakout and Plateaus says \"What's going    on    here?\"\r\n",
        "      They're having   a breakout    and    Plateaus says \"What's\r\n",
        "\r\n",
        "going on here?\"\r\n",
        "```\r\n",
        "\r\n",
        "(offset of 4, temperature .7)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw7ljx8z7Gid"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "If only I  could be my good   editor     in one piece \r\n",
        "        If only     I  could  be my good editor\r\n",
        "\r\n",
        "-- which, at least,   it   was, when  absolutely forced and wri-\r\n",
        "editor   --  which at least,    it   was, when absolutely\r\n",
        "\r\n",
        "ting   ridiculously    long chunks of crap chunks.\r\n",
        "forced and writing ridiculously       long chunks of crap\r\n",
        "\r\n",
        "Without the   ethics.          \"It sucks.\"    Sometimes,\r\n",
        "chunks.       Without  the  ethics.       \"It sucks.\"\r\n",
        "\r\n",
        "sometimes, it's just \"it's difficult       to tell a\r\n",
        "Sometimes, sometimes,      it's just \"it's difficult\r\n",
        "\r\n",
        "story\"   as art,   or an illusion.    There's another,     ever\r\n",
        "to tell  a  story\" as art, or an illusion.       There's another,\r\n",
        "\r\n",
        "another trance.  Sometimes it's \"I don't  think it's\r\n",
        "   ever another  trance.        Sometimes it's  \"I\r\n",
        "\r\n",
        "important\"         as   entertainment.\r\n",
        "don't think it's   important\" as  entertainment.\r\n",
        "```\r\n",
        "(offset of 3, temperature 2.0, vowels 4, stress 3, consonants 5)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGcVE1FaA4hj"
      },
      "source": [
        "```\r\n",
        "If only I could be allowed to  be loved.     Been  avoiding\r\n",
        "          If    only  I  could be allowed to be    loved.\r\n",
        "\r\n",
        "eye  doctor since  ninety-two.\r\n",
        "Been avoiding      eye doctor  since ninety-two.\r\n",
        "```\r\n",
        "\r\n",
        "(offset of 4, temperature 2, vowels, stress, cons = 2,1,2; 3 tokens per increment)\r\n",
        "\r\n",
        "```\r\n",
        "If only I could be that good.\r\n",
        "People say it - \"It's true it isn't\" - \"It is\", \"It is true it is true\" - \"Dear @lukejchengnott, in what way?\"\r\n",
        "```\r\n",
        "\r\n",
        "(same settings, but offset of 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAhT-dl3H8oB"
      },
      "source": [
        "```\r\n",
        "If only\r\n",
        "I could be\r\n",
        "as good as\r\n",
        "this.\r\n",
        "I know I'm\r\n",
        "doing ev\r\n",
        "ery thing right.\r\n",
        "          I\r\n",
        "know I'm do\r\n",
        "ing every\r\n",
        "thing I can.\r\n",
        "          I\r\n",
        "know I'm do\r\n",
        "ing every\r\n",
        "thing I can.\r\n",
        "          I\r\n",
        "know I'm cap\r\n",
        "able.\r\n",
        "     I be\r\n",
        "lieve I am\r\n",
        "loved.\r\n",
        "I believe\r\n",
        "I am ent\r\n",
        "ertaining\r\n",
        "the impos\r\n",
        "sible.\r\n",
        "  But all\r\n",
        "of a   sud\r\n",
        "den it seems\r\n",
        "like it's a\r\n",
        "bout to col\r\n",
        "lapse on it\r\n",
        "self and I\r\n",
        "have to find\r\n",
        "a place to\r\n",
        "stand,  so\r\n",
        "I don't lis\r\n",
        "ten.\r\n",
        "Finally\r\n",
        "I find a\r\n",
        "place to sit,\r\n",
        "   thinking\r\n",
        "it's okay\r\n",
        "to be there.\r\n",
        "      There\r\n",
        "is a rea\r\n",
        "sonable\r\n",
        "place to go,\r\n",
        "    but I'm\r\n",
        "worried it\r\n",
        "will be rai\r\n",
        "ded by the\r\n",
        "big demon\r\n",
        "ic fuckups,\r\n",
        "   and killed.\r\n",
        "          I\r\n",
        "know that I\r\n",
        "am allowed\r\n",
        "to feel my\r\n",
        "anger and\r\n",
        "anxie\r\n",
        "ty under\r\n",
        "control,\r\n",
        "that this is\r\n",
        "an extreme\r\n",
        "ly danger\r\n",
        "ous place.\r\n",
        "    I sit\r\n",
        "on it,\r\n",
        "thinking it's\r\n",
        "really o\r\n",
        "kay to be\r\n",
        "there.\r\n",
        "But then I\r\n",
        "get really\r\n",
        "worried and\r\n",
        "angry   and\r\n",
        "a     pile  of\r\n",
        "Trash piles up.\r\n",
        "```\r\n",
        "\r\n",
        "(alts -> 50 from 250, 5 tokens per increment, temperature 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77lE-sKzKLGt"
      },
      "source": [
        "(stress weight 1->200, offset -> 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ohIoeqXahkw",
        "outputId": "a72fc1ab-6b39-47f5-c87d-efac20e8d2c1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 363
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOWldxrsqVjh"
      },
      "source": [
        "kDistanceWeightVowels = 3.0\r\n",
        "kDistanceWeightStress = 3.0\r\n",
        "kDistanceWeightOnset = 2.0\r\n",
        "kDistanceWeightCoda = 5.0\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iulVyEdBa-Wr",
        "outputId": "eb0ac510-0ef5-4c65-e4b5-c62e9ea5b422"
      },
      "source": [
        "syllabify.syllabify(phones_for_word_fb(\"transmute\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['T', 'R'], ['AE0'], ['N', 'S']), (['M'], ['Y', 'UW1'], ['T'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2VxdaZ8bHZC"
      },
      "source": [
        "kDistanceWeightVowels = 3.0\r\n",
        "kDistanceWeightStress = 3.0\r\n",
        "kDistanceWeightOnset = 2.0\r\n",
        "kDistanceWeightCoda = 5.0\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBy6nfHgv7rt",
        "outputId": "d4e70ac7-711a-4e14-f3ed-f3ed3440c015"
      },
      "source": [
        "kDistanceWeightCoda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}