{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PhoneticSimilarity.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPFEJ4hs1HP7naKNRLi5fv4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThreeMachineExpression/contrapuntalPoetry/blob/main/PhoneticSimilarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPCyH2eLsbbF"
      },
      "source": [
        "\r\n",
        "Parts of this notebook are based on Max Woolf's aitextgen notebook, as updated by Allison Parrish."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxzLNBUjsxSv"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYABCBZZs2ie",
        "outputId": "82bb66b2-17ee-476f-af11-de2df8eaaa63"
      },
      "source": [
        "# Freeze versions of dependencies for now\r\n",
        "!pip install tensorflow==1.15.0 keras==2.2.5 \"h5py<3.0.0\"\r\n",
        "!pip3 install pytorch-lightning==0.7.6\r\n",
        "!pip3 install transformers==2.9.1\r\n",
        "!pip3 install fire==0.3.0\r\n",
        "\r\n",
        "!pip install -q aitextgen==0.2.3\r\n",
        "\r\n",
        "from aitextgen import aitextgen\r\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive\r\n",
        "\r\n",
        "mount_gdrive()\r\n",
        "\r\n",
        "!pip install annoy\r\n",
        "!pip install pronouncing\r\n",
        "!pip install pincelate\r\n",
        "import pronouncing\r\n",
        "import pincelate\r\n",
        "pin = pincelate.Pincelate()\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "import random\r\n",
        "import textwrap\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import logging\r\n",
        "logging.basicConfig(\r\n",
        "        format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\r\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n",
        "        level=logging.INFO\r\n",
        "    )\r\n",
        "\r\n",
        "kPhoneticSimilarityVectorsRepo = \"https://github.com/aparrish/phonetic-similarity-vectors\"\r\n",
        "\r\n",
        "!git clone {kPhoneticSimilarityVectorsRepo}\r\n",
        "\r\n",
        "!python -m spacy download en_core_web_md\r\n",
        "\r\n",
        "%cd phonetic-similarity-vectors\r\n",
        "from featurephone import phone_feature_map as pfm\r\n",
        "%cd ..\r\n",
        "\r\n",
        "pfm['STOP'] = ()\r\n",
        "\r\n",
        "vowels = ['AO','AA','IY','UW','EH','IH','UH','AH','AE','EY','AY','OW','AW','OY',\r\n",
        "          'ER','STOP']\r\n",
        "\r\n",
        "# When deciding how close two syllables are, these constants indicate how much\r\n",
        "# weight to give vowel features, stress, and consonant features\r\n",
        "kDistanceWeightVowels = 4.0\r\n",
        "kDistanceWeightStress = 5.0\r\n",
        "kDistanceWeightConsonants = 1.0\r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-lightning==0.7.6 in /usr/local/lib/python3.6/dist-packages (0.7.6)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.6) (1.7.0+cu101)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.6) (4.41.1)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.6) (1.15.0)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.6) (3.13)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.6) (0.18.2)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.6) (1.19.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch-lightning==0.7.6) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch-lightning==0.7.6) (3.7.4.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (0.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (51.3.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (0.36.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (3.3.3)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.7.6) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.7.6) (3.4.0)\n",
            "Requirement already satisfied: transformers==2.9.1 in /usr/local/lib/python3.6/dist-packages (2.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (0.7.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (0.1.95)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.1) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.1) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.1) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.1) (1.15.0)\n",
            "Requirement already satisfied: fire==0.3.0 in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire==0.3.0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire==0.3.0) (1.15.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: annoy in /usr/local/lib/python3.6/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pronouncing in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: cmudict>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from pronouncing) (0.4.5)\n",
            "Requirement already satisfied: pincelate in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: pronouncing>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from pincelate) (0.2.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from pincelate) (0.22.2.post1)\n",
            "Requirement already satisfied: Keras>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pincelate) (2.2.5)\n",
            "Requirement already satisfied: cmudict>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from pronouncing>=0.2.0->pincelate) (0.4.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->pincelate) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->pincelate) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->pincelate) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.0->pincelate) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.0->pincelate) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.0->pincelate) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.0->pincelate) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.0->pincelate) (2.10.0)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'phonetic-similarity-vectors'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Total 32 (delta 0), reused 0 (delta 0), pack-reused 32\u001b[K\n",
            "Unpacking objects: 100% (32/32), done.\n",
            "Collecting en_core_web_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp36-none-any.whl size=98051304 sha256=0729af81a74cba9ba95a67b8242f02d4390e8788dba7d598eb9016fa765fc537\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3lcml_vy/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "/content/phonetic-similarity-vectors\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZkOeQ8V4zhX"
      },
      "source": [
        "from_folder = \"aitextgenDreamFineTuning\"\r\n",
        "\r\n",
        "for file in [\"pytorch_model.bin\", \"config.json\"]:\r\n",
        "  if from_folder:\r\n",
        "    copy_file_from_gdrive(file, from_folder)\r\n",
        "  else:\r\n",
        "    copy_file_from_gdrive(file)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9afxhkk46WG",
        "outputId": "c14ca060-a695-4aa1-c6f5-8af9652cf7d4"
      },
      "source": [
        "ai = aitextgen(model=\"pytorch_model.bin\", config=\"config.json\", to_gpu=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:aitextgen:Loading GPT-2 model from provided pytorch_model.bin.\n",
            "INFO:aitextgen:Using the default GPT-2 Tokenizer.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXb_7Rin5hr-"
      },
      "source": [
        "# Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DdUOTYE5tad",
        "outputId": "c51240f2-2c5c-4855-b99d-969dcd35a44b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jan 22 09:41:04 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    39W / 300W |   1931MiB / 16130MiB |     12%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H4_V452Dy5u",
        "outputId": "766a75a3-91a9-4606-f245-440e7c955aee"
      },
      "source": [
        "ai.generate()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "You've wanted to create a story for a while only to have it leak out and into the void, where nothing real happens.\n",
            "And then it's on the news instead of the people you know.\n",
            "You try to turn it off for a while, but everyone keeps saying stupid things and it doesn't really work.\n",
            "And then it all just goes back to pretending that everything's okay and everything's okay.\n",
            "Like when the bright ball of light descends and lands in your throat and you can sing and everyone knows they're loved, the elderly women who are finally marrying feel in their bones that everything's okay and they really do love me.\n",
            "I don't need anyone to love me anymore, I need someone to love me, and I NEEDED to hear that everyone was loved so much before.\n",
            "I went to medical school so that I can provide proper care to the people I've hit over the head with 18th-century furniture.\n",
            "Everything I need to know about this job interview is in the open field outside the hospital.\n",
            "When I apply for this gig, there are rules against being late, nudity, and being a serial rapist.\n",
            "I have to get used to it.\n",
            "I don't know how to train for this\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftsVytZcEGLj"
      },
      "source": [
        "# Phonetic Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVnfSUgmEqNd"
      },
      "source": [
        "# Strategy:\r\n",
        "# Split text into syllable buckets\r\n",
        "# In each bucket, include the vowel sound, stress, and all of the bordering phonemes\r\n",
        "#  (so a consonant between syllables goes in both buckets)\r\n",
        "# Measure the distance between two buckets as \r\n",
        "#   a * (manhattan distance of consonant features)\r\n",
        "# + b * (manhattan distance of vowel features)\r\n",
        "# + c * (stress distance)\r\n",
        "# (with a, b, c tuned as desired to emphasize different similarities)\r\n",
        "#\r\n",
        "# Pauses get a bucket of their own.\r\n",
        "\r\n",
        "def phones_for_word_fb(word):\r\n",
        "  \"\"\"Phones for word - either 1st entry in the CMU pronouncing dictionary\r\n",
        "   or fallback to Pincelate sound-out if it's not in the dictionary.\r\n",
        "\r\n",
        "   Returns a list of phones.\r\n",
        "\r\n",
        "   word - lowercase word, no spaces or punctuation\r\n",
        "  \"\"\"\r\n",
        "  try:\r\n",
        "    return pronouncing.phones_for_word(word)[0].split()\r\n",
        "  except IndexError:\r\n",
        "    return pin.soundout(word)\r\n",
        "\r\n",
        "def arpabetPlusFromText(string):\r\n",
        "  \"\"\"Turns text into a list of arpabet phones, + STOP for punctuation.\r\n",
        "  One stop for ,; two stops for .:?! or double dash.\r\n",
        "  Turns a hyphen between letters (as in \"use-case\") into a space,\r\n",
        "   but in \"a phrase - like this one\" replaces it with a STOP\r\n",
        "\r\n",
        "  Discards all other punctuation and non-alpha characters (including numbers).\r\n",
        "\r\n",
        "  string -- text to convert to phones\"\"\"\r\n",
        "\r\n",
        "  lidx = ridx = 0\r\n",
        "  \r\n",
        "  phones = []\r\n",
        "  doublestops = \".:?!\"\r\n",
        "  singlestops = \",;\"\r\n",
        "\r\n",
        "  while (lidx < len(string)):\r\n",
        "    if string[lidx].isalpha():\r\n",
        "      while ((ridx < len(string)) and (string[ridx].isalpha())):\r\n",
        "        ridx += 1\r\n",
        "      phones += phones_for_word_fb(string[lidx:ridx].lower())\r\n",
        "      lidx = ridx\r\n",
        "    else:\r\n",
        "      while ((ridx < len(string)) and (not string[ridx].isalpha())):\r\n",
        "        ridx += 1\r\n",
        "      doublestop = False\r\n",
        "      for c in doublestops:\r\n",
        "        if c in string[lidx:ridx]:\r\n",
        "          doublestop = True\r\n",
        "      if doublestop:\r\n",
        "        phones += ['STOP0','STOP0']\r\n",
        "      elif '--' in string[lidx:ridx]:\r\n",
        "        phones += ['STOP0','STOP0']\r\n",
        "      elif '- ' in string[lidx:ridx]:\r\n",
        "        phones += ['STOP0']\r\n",
        "      else:\r\n",
        "        singlestop = False\r\n",
        "        for c in singlestops:\r\n",
        "          if c in string[lidx:ridx]:\r\n",
        "            singlestop = True\r\n",
        "        if singlestop:\r\n",
        "          phones += ['STOP0']\r\n",
        "      lidx = ridx\r\n",
        "\r\n",
        "  return phones\r\n",
        "\r\n",
        "class Syllable:\r\n",
        "  # initialization takes a list of phones\r\n",
        "  # assuming exactly one vowel/stop in them.\r\n",
        "  #\r\n",
        "  # vowel - vowel phone (w/o stress indicator) or STOP\r\n",
        "  # stress - 0 for STOP, 1 for unstressed, 2 2ndary stress, 3 primary stress\r\n",
        "  # (note this is different from the arpabet numbers)\r\n",
        "  # cfeat - Counter containing a count of the consonant features\r\n",
        "  # vfeat - Counter containing a count of the vowel features\r\n",
        "  def __init__(self, phoneslist):\r\n",
        "    phones = \" \".join(phoneslist)\r\n",
        "    for v in vowels:\r\n",
        "      loc = phones.find(v)\r\n",
        "      if (loc != -1):\r\n",
        "        self.vowel = v\r\n",
        "        if (v == 'STOP'):\r\n",
        "          self.stress = 0\r\n",
        "        elif (phones[loc + 2] == '0'):\r\n",
        "          self.stress = 1\r\n",
        "        elif (phones[loc + 2] == '1'):\r\n",
        "          self.stress = 3\r\n",
        "        elif (phones[loc + 2] == '2'):\r\n",
        "          self.stress = 2\r\n",
        "      \r\n",
        "      self.cfeat = Counter()\r\n",
        "      self.vfeat = Counter()\r\n",
        "      for p in phoneslist:\r\n",
        "        if isVowel(p):\r\n",
        "          for feature in pfm[p[0:-1]]:\r\n",
        "            self.vfeat[feature] += 1\r\n",
        "        else:\r\n",
        "          for feature in pfm[p]:\r\n",
        "            self.cfeat[feature] += 1\r\n",
        "                \r\n",
        "def distance(syl1 : Syllable, syl2 : Syllable):\r\n",
        "  \"\"\"A distance metric between two Syllable objects.\r\n",
        "  Adjusted by the constant weights in the first cell,\r\n",
        "  applied to Manhattan distance on features.\r\n",
        "  (STOP is close to every other vowel in features,\r\n",
        "  though not in stress)\r\n",
        "  \"\"\"\r\n",
        "  cdist = 0\r\n",
        "  vdist = 0\r\n",
        "  \r\n",
        "  sdist = abs(syl1.stress - syl2.stress) * kDistanceWeightStress\r\n",
        "\r\n",
        "  cdiff = difference(syl1.cfeat,syl2.cfeat)\r\n",
        "  for f in cdiff:\r\n",
        "    cdist += abs(cdiff[f]) * kDistanceWeightConsonants\r\n",
        "  \r\n",
        "  if (syl1.vowel != 'STOP' and syl2.vowel != 'STOP'):\r\n",
        "    vdiff = difference(syl1.vfeat,syl2.vfeat)\r\n",
        "    for f in vdiff:\r\n",
        "      vdist += abs(vdiff[f]) * kDistanceWeightVowels\r\n",
        "\r\n",
        "  return sdist + vdist + cdist\r\n",
        "\r\n",
        "def difference(ctr1 : Counter, ctr2 : Counter):\r\n",
        "  \"\"\"A new counter that's the difference between two Counters.\r\n",
        "  (Counter.subtract mutates the Counter, which isn't what we want.)\r\n",
        "  \"\"\"\r\n",
        "  diff = Counter(ctr1)\r\n",
        "  for c in ctr2:\r\n",
        "    diff[c] -= ctr2[c]\r\n",
        "  \r\n",
        "  return diff\r\n",
        "\r\n",
        "def isVowel(phone):\r\n",
        "  return phone[0:-1] in vowels\r\n",
        "  \r\n",
        "def syllabifier(phoneslist):\r\n",
        "  \"\"\"Turns a list of phones into a list of Syllable objects.\"\"\"\r\n",
        "\r\n",
        "  i = j = nextSylStart = 0\r\n",
        "  syllableList = []\r\n",
        "\r\n",
        "  while i < len(phoneslist):\r\n",
        "    thisSyllablePhones = []\r\n",
        "    j = i\r\n",
        "    needVowel = True\r\n",
        "    \r\n",
        "    while j < len(phoneslist) and (needVowel or not isVowel(phoneslist[j])):\r\n",
        "      if isVowel(phoneslist[j]):\r\n",
        "        needVowel = False\r\n",
        "        nextSylStart = j + 1\r\n",
        "      thisSyllablePhones.append(phoneslist[j])\r\n",
        "      \r\n",
        "      j = j + 1\r\n",
        "    \r\n",
        "    # only add a syllable if we picked up a vowel\r\n",
        "    if not needVowel:\r\n",
        "      syllableList.append(Syllable(thisSyllablePhones))\r\n",
        "      i = nextSylStart\r\n",
        "    else:\r\n",
        "      i = j\r\n",
        "\r\n",
        "  return syllableList\r\n",
        "\r\n",
        "def canonFit(list1, list2, offset):\r\n",
        "  \"\"\"Given two syllable lists and a canon offset,\r\n",
        "  returns the total syllable distance. Lower is rhymier.\r\n",
        "  list2 starts offset syllables later.\"\"\"\r\n",
        "\r\n",
        "  i = totalDist = 0\r\n",
        "  while (i + offset < len(list1) and i < len(list2)):\r\n",
        "    totalDist += distance(list2[i], list1[i + offset])\r\n",
        "    i = i + 1\r\n",
        "  \r\n",
        "  return totalDist"
      ],
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcjQ0BKKELvx"
      },
      "source": [
        "def generateCanon(alts = 100, tokensPerIncrement = 5, length = 100,\r\n",
        "                  prompt = \"If only I could be a little more\",\r\n",
        "                  temperature = 1.2, top_p = 0.9, offset = 10,\r\n",
        "                  startingTokens=10):\r\n",
        "  \"\"\"\r\n",
        "  alts - number of samples of text to generate to test for rhymes\r\n",
        "  tokensPerIncrement - size of each incremental sample\r\n",
        "  length - total tokens (including the prompt)\r\n",
        "  prompt - start of canon\r\n",
        "  temperature, top_p - passed along to GPT-2\r\n",
        "  offset - how many syllables the 2nd part of the canon is delayed\r\n",
        "  startingTokens - tokens for first generation. Should be more tokens than the\r\n",
        "    prompt (tricky, because it's not clear how many tokens the prompt will contain)\r\n",
        "  \"\"\"\r\n",
        "  print(prompt)\r\n",
        "  canonSoFar = prompt\r\n",
        "  seed = 0\r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "  for i in range(startingTokens, length, tokensPerIncrement):\r\n",
        "    canonSoFarSyllables = syllabifier(arpabetPlusFromText(canonSoFar))  \r\n",
        "    bestTrial = canonSoFar\r\n",
        "    bestDist = float(\"inf\")\r\n",
        "\r\n",
        "    for j in range(alts):\r\n",
        "      seed += 1\r\n",
        "      random.seed(seed)\r\n",
        "      trial = ai.generate_one(\r\n",
        "              prompt=canonSoFar,\r\n",
        "              max_length=i,\r\n",
        "              temperature=temperature,\r\n",
        "              top_p=top_p)\r\n",
        "      trialDiff = trial[len(canonSoFar):]\r\n",
        "      print(\"next text: \" + trialDiff)\r\n",
        "      trialSyllables = canonSoFarSyllables + syllabifier(arpabetPlusFromText(trialDiff))\r\n",
        "      if len(trialSyllables) > len(canonSoFarSyllables): # only test generations with real new text\r\n",
        "        trialFit = canonFit(trialSyllables, canonSoFarSyllables, offset)\r\n",
        "        if trialFit < bestDist:\r\n",
        "          bestDist = trialFit\r\n",
        "          bestTrial = trial\r\n",
        "    \r\n",
        "    canonSoFar = bestTrial\r\n",
        "    print(canonSoFar)\r\n",
        "\r\n",
        "  print()\r\n",
        "  print(\"FINAL CANON:\")\r\n",
        "  print(canonSoFar)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VMJq4r23Iolk",
        "outputId": "8b9ccf95-211a-4577-9d24-696ff11fb5ff"
      },
      "source": [
        "generateCanon(alts = 50, tokensPerIncrement=2, length=200, prompt=\"If only I could be\", offset = 4, temperature = 1, top_p = 0.9)\r\n"
      ],
      "execution_count": 367,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "If only I could be\n",
            "next text:  like her.\n",
            "That\n",
            "next text:  like this.\n",
            "Start\n",
            "next text:  beautiful.\n",
            "Doing\n",
            "next text:  beautiful.\n",
            "Let's\n",
            "next text:  your father, a mom\n",
            "next text:  so beautiful.\n",
            "You\n",
            "next text:  saved, but it would\n",
            "next text:  saved!\n",
            "I don\n",
            "next text:  your father.\n",
            "Oh\n",
            "next text:  this beautiful.\n",
            "I\n",
            "next text:  my real self today.\n",
            "next text:  like him.\n",
            "I\n",
            "next text:  my best self, so\n",
            "next text:  the one to watch the\n",
            "next text:  as beautiful as this.\n",
            "next text:  like her.\n",
            "Like\n",
            "next text:  as beautiful as this.\n",
            "next text:  the one to ask for\n",
            "next text:  your father, too.\n",
            "next text:  allowed to have that conversation\n",
            "next text:  saved.\n",
            "I can\n",
            "next text:  my best self and not\n",
            "next text:  in that boat.\n",
            "\n",
            "next text:  saved.\n",
            "I'm\n",
            "next text:  so beautiful.\n",
            "And\n",
            "next text:  saved.\n",
            "I'd\n",
            "next text:  like her.\n",
            "Like\n",
            "next text:  like him.\n",
            "Like\n",
            "next text:  your father today.\n",
            "\n",
            "next text:  beautiful.\n",
            "And I\n",
            "next text:  yourself, a little closer\n",
            "next text:  my best self and not\n",
            "next text:  as beautiful as this!\n",
            "next text:  beautiful.\n",
            "And all\n",
            "next text:  your friend.\n",
            "That\n",
            "next text:  as beautiful as this.\n",
            "next text:  saved.\n",
            "Life is\n",
            "next text:  here today, and not\n",
            "next text:  saving face for his funeral\n",
            "next text:  the one to buy them\n",
            "next text:  in that position a little\n",
            "next text:  myself a little.\n",
            "\n",
            "next text:  as beautiful as this!\n",
            "next text:  in a dressing room with\n",
            "next text:  more careful.\n",
            "Maybe\n",
            "next text:  myself a little bit.\n",
            "next text:  more like her.\n",
            "\n",
            "next text:  so beautiful and so free\n",
            "next text:  their personal doctor.\n",
            "\n",
            "next text:  beautiful.\n",
            "And not\n",
            "If only I could be in that boat.\n",
            "\n",
            "next text: I might\n",
            "next text: It's\n",
            "next text: I might\n",
            "next text: Hey,\n",
            "next text: Maybe I\n",
            "next text: It's\n",
            "next text: Mom and\n",
            "next text: You see\n",
            "next text: Maybe.\n",
            "next text: I'm\n",
            "next text: Yeah.\n",
            "next text: I don\n",
            "next text: No one\n",
            "next text: No boat\n",
            "next text: Been\n",
            "next text: I'd\n",
            "next text: I can\n",
            "next text: That boat\n",
            "next text: Maybe I\n",
            "next text: Cruel\n",
            "next text: Oh,\n",
            "next text: Something went\n",
            "next text: It's\n",
            "next text: No boat\n",
            "next text: No boat\n",
            "next text: Hey,\n",
            "next text: Anna Johnson\n",
            "next text: It's\n",
            "next text: Maybe I\n",
            "next text: That's\n",
            "next text: What a\n",
            "next text: I'm\n",
            "next text: I would\n",
            "next text: I could\n",
            "next text: I'd\n",
            "next text: Oh,\n",
            "next text: Start over\n",
            "next text: I'd\n",
            "next text: Oh,\n",
            "next text: Maybe the\n",
            "next text: You know\n",
            "next text: It's\n",
            "next text: I could\n",
            "next text: Cruel\n",
            "next text: Yeah,\n",
            "next text: I might\n",
            "next text: I don\n",
            "next text: Anna Johnson\n",
            "next text: Eventually everyone\n",
            "next text: I know\n",
            "If only I could be in that boat.\n",
            "That boat\n",
            "next text: 's always\n",
            "next text:  needs people\n",
            "next text:  could have\n",
            "next text:  would be\n",
            "next text: , too\n",
            "next text: .\n",
            "\n",
            "next text: , you\n",
            "next text: .\n",
            "\n",
            "next text: .\n",
            "\n",
            "next text: ?\n",
            "\n",
            "next text: .\n",
            "\n",
            "next text: !\n",
            "\n",
            "next text: .\n",
            "\n",
            "next text:  needs people\n",
            "next text: .\n",
            "\n",
            "next text:  would have\n",
            "next text: .\n",
            "\n",
            "next text:  couldn't\n",
            "next text:  couldn't\n",
            "next text:  could save\n",
            "next text: .\n",
            "\n",
            "next text: .\n",
            "\n",
            "next text: 's always\n",
            "next text: .\n",
            "\n",
            "next text: .\n",
            "\n",
            "next text: ?\n",
            "\n",
            "next text: .\n",
            "\n",
            "next text: .\n",
            "\n",
            "next text: , that\n",
            "next text:  needs people\n",
            "next text: .\n",
            "\n",
            "next text: 's always\n",
            "next text: , that\n",
            "next text: .\n",
            "\n",
            "next text: , this\n",
            "next text: .\n",
            "\n",
            "next text: , people\n",
            "next text: .\n",
            "\n",
            "next text: .\n",
            "\n",
            "next text: .\n",
            "\n",
            "next text: .\n",
            "\n",
            "next text:  would have\n",
            "next text: .\n",
            "\n",
            "next text:  could be\n",
            "next text: , now\n",
            "next text: 's not\n",
            "next text:  could have\n",
            "next text: .\n",
            "\n",
            "next text: .\n",
            "\n",
            "next text: .\n",
            "\n",
            "If only I could be in that boat.\n",
            "That boat.\n",
            "\n",
            "next text: Dad needs\n",
            "next text: Let's\n",
            "next text: It has\n",
            "next text: Oh,\n",
            "next text: DON'\n",
            "next text: Hold on\n",
            "next text: Wherever\n",
            "next text: You know\n",
            "next text: We can\n",
            "next text: Or that\n",
            "next text: There's\n",
            "next text: We might\n",
            "next text: We could\n",
            "next text: DON'\n",
            "next text: Skates\n",
            "next text: I'm\n",
            "next text: Everyone wants\n",
            "next text: Dad needs\n",
            "next text: Hold on\n",
            "next text: Hold on\n",
            "next text: When they\n",
            "next text: Mom needs\n",
            "next text: I am\n",
            "next text: I would\n",
            "next text: Mom and\n",
            "next text: I thought\n",
            "next text: Cig\n",
            "next text: We'd\n",
            "next text: Mom and\n",
            "next text: The one\n",
            "next text: What a\n",
            "next text: You know\n",
            "next text: Cou\n",
            "next text: Oh,\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-367-70460a344114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerateCanon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokensPerIncrement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"If only I could be\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-366-31b76b1c888d>\u001b[0m in \u001b[0;36mgenerateCanon\u001b[0;34m(alts, tokensPerIncrement, length, prompt, temperature, top_p, offset, startingTokens)\u001b[0m\n\u001b[1;32m     31\u001b[0m               \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m               \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m               top_p=top_p)\n\u001b[0m\u001b[1;32m     34\u001b[0m       \u001b[0mtrialDiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanonSoFar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"next text: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrialDiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/aitextgen/aitextgen.py\u001b[0m in \u001b[0;36mgenerate_one\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \"\"\"\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_as_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     def generate_samples(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/aitextgen/aitextgen.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, n, prompt, max_length, temperature, do_sample, return_as_list, seed, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         )\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_specific_kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m             )\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_generate_no_beam_search\u001b[0;34m(self, input_ids, cur_len, max_length, min_length, do_sample, temperature, top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, decoder_start_token_id, batch_size, encoder_outputs, attention_mask, use_cache, model_specific_kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m             )\n\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache)\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m         )\n\u001b[1;32m    617\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache)\u001b[0m\n\u001b[1;32m    497\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m             )\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, use_cache)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_attn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# output_attn: a, present, (attentions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, use_cache)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mpresent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mattn_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36m_attn\u001b[0;34m(self, q, k, v, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mnd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnd\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WH8dvGm2miz"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "If  only I  could be in that  boat.       That boat.\r\n",
        "            If    only  I     could be in that boat.\r\n",
        "\r\n",
        "I    don't know why.       Cousinwog, cousin of  Brassy and\r\n",
        "That boat.         I don't know why.  Cousinwog,     cousin \r\n",
        "\r\n",
        "Red from    Cousinwog  and   Mighty from   Cousinwog  \r\n",
        "of  Brassy     and Red from  Cousinwog    and Mighty from\r\n",
        "\r\n",
        "City  and  Plateaus  aren't  together        anymore.   They're\r\n",
        "Cousinwog  City      and     Plateaus aren't together anymore. \r\n",
        "\r\n",
        "having a breakout and Plateaus says \"What's going    on    here?\"\r\n",
        "      They're having   a breakout    and    Plateaus says \"What's\r\n",
        "\r\n",
        "going on here?\"\r\n",
        "```\r\n",
        "\r\n",
        "(offset of 4, temperature .7)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw7ljx8z7Gid"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "If only I  could be my good   editor     in one piece \r\n",
        "        If only     I  could  be my good editor\r\n",
        "\r\n",
        "-- which, at least,   it   was, when  absolutely forced and wri-\r\n",
        "editor   --  which at least,    it   was, when absolutely\r\n",
        "\r\n",
        "ting   ridiculously    long chunks of crap chunks.\r\n",
        "forced and writing ridiculously       long chunks of crap\r\n",
        "\r\n",
        "Without the   ethics.          \"It sucks.\"    Sometimes,\r\n",
        "chunks.       Without  the  ethics.       \"It sucks.\"\r\n",
        "\r\n",
        "sometimes, it's just \"it's difficult       to tell a\r\n",
        "Sometimes, sometimes,      it's just \"it's difficult\r\n",
        "\r\n",
        "story\"   as art,   or an illusion.    There's another,     ever\r\n",
        "to tell  a  story\" as art, or an illusion.       There's another,\r\n",
        "\r\n",
        "another trance.  Sometimes it's \"I don't  think it's\r\n",
        "   ever another  trance.        Sometimes it's  \"I\r\n",
        "\r\n",
        "important\"         as   entertainment.\r\n",
        "don't think it's   important\" as  entertainment.\r\n",
        "```\r\n",
        "(offset of 3, temperature 2.0, vowels 4, stress 3, consonants 5)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGcVE1FaA4hj"
      },
      "source": [
        "```\r\n",
        "If only I could be allowed to  be loved.     Been  avoiding\r\n",
        "          If    only  I  could be allowed to be    loved.\r\n",
        "\r\n",
        "eye  doctor since  ninety-two.\r\n",
        "Been avoiding      eye doctor  since ninety-two.\r\n",
        "```\r\n",
        "\r\n",
        "(offset of 4, temperature 2, vowels, stress, cons = 2,1,2; 3 tokens per increment)\r\n",
        "\r\n",
        "```\r\n",
        "If only I could be that good.\r\n",
        "People say it - \"It's true it isn't\" - \"It is\", \"It is true it is true\" - \"Dear @lukejchengnott, in what way?\"\r\n",
        "```\r\n",
        "\r\n",
        "(same settings, but offset of 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAhT-dl3H8oB"
      },
      "source": [
        "```\r\n",
        "If only\r\n",
        "I could be\r\n",
        "as good as\r\n",
        "this.\r\n",
        "I know I'm\r\n",
        "doing ev\r\n",
        "ery thing right.\r\n",
        "          I\r\n",
        "know I'm do\r\n",
        "ing every\r\n",
        "thing I can.\r\n",
        "          I\r\n",
        "know I'm do\r\n",
        "ing every\r\n",
        "thing I can.\r\n",
        "          I\r\n",
        "know I'm cap\r\n",
        "able.\r\n",
        "     I be\r\n",
        "lieve I am\r\n",
        "loved.\r\n",
        "I believe\r\n",
        "I am ent\r\n",
        "ertaining\r\n",
        "the impos\r\n",
        "sible.\r\n",
        "  But all\r\n",
        "of a   sud\r\n",
        "den it seems\r\n",
        "like it's a\r\n",
        "bout to col\r\n",
        "lapse on it\r\n",
        "self and I\r\n",
        "have to find\r\n",
        "a place to\r\n",
        "stand,  so\r\n",
        "I don't lis\r\n",
        "ten.\r\n",
        "Finally\r\n",
        "I find a\r\n",
        "place to sit,\r\n",
        "   thinking\r\n",
        "it's okay\r\n",
        "to be there.\r\n",
        "      There\r\n",
        "is a rea\r\n",
        "sonable\r\n",
        "place to go,\r\n",
        "    but I'm\r\n",
        "worried it\r\n",
        "will be rai\r\n",
        "ded by the\r\n",
        "big demon\r\n",
        "ic fuckups,\r\n",
        "   and killed.\r\n",
        "          I\r\n",
        "know that I\r\n",
        "am allowed\r\n",
        "to feel my\r\n",
        "anger and\r\n",
        "anxie\r\n",
        "ty under\r\n",
        "control,\r\n",
        "that this is\r\n",
        "an extreme\r\n",
        "ly danger\r\n",
        "ous place.\r\n",
        "    I sit\r\n",
        "on it,\r\n",
        "thinking it's\r\n",
        "really o\r\n",
        "kay to be\r\n",
        "there.\r\n",
        "But then I\r\n",
        "get really\r\n",
        "worried and\r\n",
        "angry   and\r\n",
        "a     pile  of\r\n",
        "Trash piles up.\r\n",
        "```\r\n",
        "\r\n",
        "(alts -> 50 from 250, 5 tokens per increment, temperature 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77lE-sKzKLGt"
      },
      "source": [
        "(stress weight 1->200, offset -> 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ohIoeqXahkw",
        "outputId": "a72fc1ab-6b39-47f5-c87d-efac20e8d2c1"
      },
      "source": [
        "kDistanceWeightVowels = 2.0\r\n",
        "kDistanceWeightStress = 5.0\r\n",
        "kDistanceWeightConsonants = 5.0\r\n",
        "\r\n",
        "s = syllabifier(arpabetPlusFromText(\"the road the road the road the road\"))\r\n",
        "canonFit(s,s,2)"
      ],
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 363
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC8S25A6C-tP"
      },
      "source": [
        "todo: don't choke on accented characters."
      ]
    }
  ]
}