{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PhoneticSimilarity.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM9s/DscnlrdV0yd3cUCh3I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThreeMachineExpression/contrapuntalPoetry/blob/main/PhoneticSimilarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPCyH2eLsbbF"
      },
      "source": [
        "\r\n",
        "Parts of this notebook are based on Max Woolf's aitextgen notebook, as updated by Allison Parrish.\r\n",
        "\r\n",
        "Also makes use of Kyle Gorman's syllabification library, syllabify.\r\n",
        "\r\n",
        "Initial implementation looked for slant rhyme / slant alliteration by grabbing all of the consonants from the boundaries between vowels and summing their features, but that leads to too-mushy results. The syllabification approach misses assonance that crosses syllable boundaries - a todo is to figure out how to bring some of that back.\r\n",
        "\r\n",
        "Todo: correctly handle single letters (as in e.g.). It sounds them out correctly, but adds two STOPs between each for the periods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxzLNBUjsxSv"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYABCBZZs2ie",
        "outputId": "d0c2b273-8be4-4e85-e380-c1f5ae354a3a"
      },
      "source": [
        "# Freeze versions of dependencies for now\r\n",
        "!pip install tensorflow==1.15.0 keras==2.2.5 \"h5py<3.0.0\"\r\n",
        "!pip3 install pytorch-lightning==0.7.6\r\n",
        "!pip3 install transformers==2.9.1\r\n",
        "!pip3 install fire==0.3.0\r\n",
        "\r\n",
        "!pip install -q aitextgen==0.2.3\r\n",
        "\r\n",
        "from aitextgen import aitextgen\r\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive\r\n",
        "\r\n",
        "mount_gdrive()\r\n",
        "\r\n",
        "!pip install annoy\r\n",
        "!pip install pronouncing\r\n",
        "!pip install pincelate\r\n",
        "import pronouncing\r\n",
        "import pincelate\r\n",
        "pin = pincelate.Pincelate()\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "import random\r\n",
        "import textwrap\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import logging\r\n",
        "logging.basicConfig(\r\n",
        "        format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\r\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n",
        "        level=logging.INFO\r\n",
        "    )\r\n",
        "\r\n",
        "kPhoneticSimilarityVectorsRepo = \"https://github.com/aparrish/phonetic-similarity-vectors\"\r\n",
        "\r\n",
        "!git clone {kPhoneticSimilarityVectorsRepo}\r\n",
        "\r\n",
        "!python -m spacy download en_core_web_md\r\n",
        "\r\n",
        "%cd phonetic-similarity-vectors\r\n",
        "from featurephone import phone_feature_map as pfm\r\n",
        "%cd ..\r\n",
        "\r\n",
        "kSyllabifyRepo = \"https://github.com/threemachineexpression/syllabify\"\r\n",
        "!git clone {kSyllabifyRepo}\r\n",
        "\r\n",
        "%cd syllabify\r\n",
        "import syllabify\r\n",
        "%cd .."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 28kB/s \n",
            "\u001b[?25hCollecting keras==2.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/ba/2d058dcf1b85b9c212cc58264c98a4a7dd92c989b798823cc5690d062bb2/Keras-2.2.5-py2.py3-none-any.whl (336kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 44.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py<3.0.0 in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.36.2)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 49.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.19.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0) (51.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=91498d9cc11a3684b59fce5857da7fcd550f06c4475b4079fba10ef7d060ae25\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, gast, tensorflow-estimator, tensorboard, tensorflow, keras\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.0\n",
            "    Uninstalling tensorboard-2.4.0:\n",
            "      Successfully uninstalled tensorboard-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.0\n",
            "    Uninstalling tensorflow-2.4.0:\n",
            "      Successfully uninstalled tensorflow-2.4.0\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed gast-0.2.2 keras-2.2.5 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n",
            "Collecting pytorch-lightning==0.7.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/ab/561d1fa6e5af30b2fd7cb4001f93eb08531e1b72976f13eebf7f7cdc021c/pytorch_lightning-0.7.6-py3-none-any.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.6) (4.41.1)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.6) (1.15.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.6) (1.7.0+cu101)\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.6) (3.13)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.6) (1.19.5)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (0.36.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (0.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (51.3.3)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (1.32.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (3.3.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch-lightning==0.7.6) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch-lightning==0.7.6) (0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.7.6) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.7.6) (3.4.0)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=7f158d2cb668f2fa9ecd10253fab4e53a166359932a3f0e560922f01cf016a00\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built future\n",
            "Installing collected packages: future, pytorch-lightning\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed future-0.18.2 pytorch-lightning-0.7.6\n",
            "Collecting transformers==2.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 4.3MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 20.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 49.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.1) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.1) (2020.12.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=fef841488c38fda3a42897e6db76c5bbbbdf66be403cb837ac32df98dad4bf8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.7.0 transformers-2.9.1\n",
            "Collecting fire==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/42/6252075cbad90b9efb27b6586827779758c62fd73868a1cd0d23ebb5aac6/fire-0.3.0.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire==0.3.0) (1.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.3.0-py2.py3-none-any.whl size=111108 sha256=da216c60dd1cef16fd1764e2158d3cf6fd1c372f4e5007c31417972a5c11933e\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/62/79/6a40acd827ec9d78d610be311820ecf8e41db024d8b1d12ace\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.3.0\n",
            "\u001b[K     |████████████████████████████████| 573kB 4.0MB/s \n",
            "\u001b[?25h  Building wheel for aitextgen (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Mounted at /content/drive\n",
            "Collecting annoy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/5b/1c22129f608b3f438713b91cd880dc681d747a860afe3e8e0af86e921942/annoy-1.17.0.tar.gz (646kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 6.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp36-cp36m-linux_x86_64.whl size=390347 sha256=f4d526b912c8dfe74de07da02874076ac82cabbcade452123fcec32540e1eb29\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/c5/59/cce7e67b52c8e987389e53f917b6bb2a9d904a03246fadcb1e\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n",
            "Collecting pronouncing\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/c6/9dc74a3ddca71c492e224116b6654592bfe5717b4a78582e4d9c3345d153/pronouncing-0.2.0.tar.gz\n",
            "Collecting cmudict>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/77/f009abf803286876fa99cb7bd9d1132c7b64a0b34a0360666275ce1bc733/cmudict-0.4.5-py2.py3-none-any.whl (939kB)\n",
            "\u001b[K     |████████████████████████████████| 942kB 5.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pronouncing\n",
            "  Building wheel for pronouncing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pronouncing: filename=pronouncing-0.2.0-py2.py3-none-any.whl size=6223 sha256=55a7d192377702a8214b166f0ffe24ddd557e9b50e97611384a6c3e0a700ac2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/fd/e8/fb1a226f707c7e20dbed4c43f81b819d279ffd3b0e2f06ee13\n",
            "Successfully built pronouncing\n",
            "Installing collected packages: cmudict, pronouncing\n",
            "Successfully installed cmudict-0.4.5 pronouncing-0.2.0\n",
            "Collecting pincelate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/b0/b8c6101e3e643cd712f42e8a5b4fb01dd5c3255c975f66d6673386341caa/pincelate-0.0.1-py3-none-any.whl (12.0MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0MB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from pincelate) (0.22.2.post1)\n",
            "Requirement already satisfied: Keras>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pincelate) (2.2.5)\n",
            "Requirement already satisfied: pronouncing>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from pincelate) (0.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->pincelate) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->pincelate) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->pincelate) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.0->pincelate) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.0->pincelate) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.0->pincelate) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.0->pincelate) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.0->pincelate) (3.13)\n",
            "Requirement already satisfied: cmudict>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from pronouncing>=0.2.0->pincelate) (0.4.5)\n",
            "Installing collected packages: pincelate\n",
            "Successfully installed pincelate-0.0.1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'phonetic-similarity-vectors'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Total 32 (delta 0), reused 0 (delta 0), pack-reused 32\u001b[K\n",
            "Unpacking objects: 100% (32/32), done.\n",
            "Collecting en_core_web_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp36-none-any.whl size=98051304 sha256=2ca5ad03e7057ed1701147eff9cc324b8ccce1743514e62345db2878f5be0969\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tuf9uc4_/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "/content/phonetic-similarity-vectors\n",
            "/content\n",
            "Cloning into 'syllabify'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 62 (delta 3), reused 0 (delta 0), pack-reused 53\u001b[K\n",
            "Unpacking objects: 100% (62/62), done.\n",
            "/content/syllabify\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZkOeQ8V4zhX"
      },
      "source": [
        "from_folder = \"aitextgenDreamFineTuning\"\r\n",
        "\r\n",
        "for file in [\"pytorch_model.bin\", \"config.json\"]:\r\n",
        "  if from_folder:\r\n",
        "    copy_file_from_gdrive(file, from_folder)\r\n",
        "  else:\r\n",
        "    copy_file_from_gdrive(file)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9afxhkk46WG",
        "outputId": "172fc931-c588-4043-ad75-4ba619409f94"
      },
      "source": [
        "ai = aitextgen(model=\"pytorch_model.bin\", config=\"config.json\", to_gpu=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:aitextgen:Loading GPT-2 model from provided pytorch_model.bin.\n",
            "INFO:aitextgen:Using the default GPT-2 Tokenizer.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXb_7Rin5hr-"
      },
      "source": [
        "# Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DdUOTYE5tad",
        "outputId": "ef7ed54f-7294-45a9-ad3d-2675d214b53d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jan 24 02:49:38 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    40W / 300W |  16087MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H4_V452Dy5u",
        "outputId": "53cbae9b-445f-4de4-dc82-d391c2593a70"
      },
      "source": [
        "ai.generate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "You've made my night so much easier.\n",
            "Oh, I'm so sorry.\n",
            "I'm so sorry.\n",
            "I'm so sorry.\n",
            "I know you didn't mean it, but I know you didn't.\n",
            "I was just so sorry I couldn't get out from under you.\n",
            "And I could be kicked out of Eastman for it, but really I could be kicked out for anything, I'm already sneezing the wrong way, I'm starving, and having a really bad case of scabies, and I wake up the next morning thinking I've been impregnated, and it wouldn't have happened if I hadn't had the thought that what you were thinking.\n",
            "So I'm trying really hard to think of what I'm going to say to get out of this, to make sure no one sees it, to make sure that no one sees my weaknesses, to make sure that the people who love me don't hurt me when I say it, to make sure that the damage isn't irreparable.\n",
            "I'm trying really hard to pretend that I don't know anything about this kid, that I know how special this kid is and how talented this kid is, that I don't have to see his face in real\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftsVytZcEGLj"
      },
      "source": [
        "# Phonetic Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVnfSUgmEqNd"
      },
      "source": [
        "# Strategy:\r\n",
        "# Split text into syllable buckets\r\n",
        "# In each bucket, include the vowel sound, stress, and all of the bordering phonemes\r\n",
        "#  (so a consonant between syllables goes in both buckets)\r\n",
        "# Measure the distance between two buckets as \r\n",
        "#   a * (manhattan distance of consonant features)\r\n",
        "# + b * (manhattan distance of vowel features)\r\n",
        "# + c * (stress distance)\r\n",
        "# (with a, b, c tuned as desired to emphasize different similarities)\r\n",
        "#\r\n",
        "# Pauses get a bucket of their own.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class Syllable:\r\n",
        "  # initialization takes a list of 3 lists of strings:\r\n",
        "  # the phones for onset, nucleus, and coda\r\n",
        "  # (as per the output of syllabify).\r\n",
        "  #\r\n",
        "  # vowel - vowel phone (w/o stress indicator) or STOP\r\n",
        "  # stress - 0 for STOP, 1 for unstressed, 2 2ndary stress, 3 primary stress\r\n",
        "  # (note this is different from the arpabet numbers)\r\n",
        "  # ofeat - Counter containing a count of the consonant features in the onset\r\n",
        "  # cfeat - Counter containing a count of the consonant features in the coda\r\n",
        "  # vfeat - Counter containing a count of the vowel features\r\n",
        "  def __init__(self, syllableList):\r\n",
        "    self.vowel = syllableList[1][0][0:-1]\r\n",
        "\r\n",
        "    # convert arpabet stress to a representation capable of similarity math\r\n",
        "    stressConversion = {\r\n",
        "        '0': 1,\r\n",
        "        '1': 3,\r\n",
        "        '2': 2\r\n",
        "    }\r\n",
        "\r\n",
        "    if self.vowel == 'STOP':\r\n",
        "      self.stress = 0\r\n",
        "    else:\r\n",
        "      self.stress = stressConversion[syllableList[1][0][-1]]\r\n",
        "      \r\n",
        "    self.ofeat = Counter()\r\n",
        "    self.cfeat = Counter()\r\n",
        "    self.vfeat = Counter()\r\n",
        "\r\n",
        "    for feature in pfm[self.vowel]:\r\n",
        "      self.vfeat[feature] += 1\r\n",
        "    \r\n",
        "    for phone in syllableList[0]:\r\n",
        "      for feature in pfm[phone]:\r\n",
        "        self.ofeat[feature] += 1\r\n",
        "    \r\n",
        "    for phone in syllableList[2]:\r\n",
        "      for feature in pfm[phone]:\r\n",
        "        self.cfeat[feature] += 1\r\n",
        "\r\n",
        "pfm['STOP'] = ()\r\n",
        "stopSyllable = Syllable(([],['STOP0'],[]))\r\n",
        "\r\n",
        "vowels = ('AO','AA','IY','UW','EH','IH','UH','AH','AE','EY','AY','OW','AW','OY',\r\n",
        "          'ER','STOP')\r\n",
        "consonantFeatures = ('alv','apr','asp','blb','dnt','frc','glt','lat','lbd','lbv','nas','pal','pla','stp','vcd','vel','vls')\r\n",
        "vowelFeatures = ('bck','cnt','fnt','hgh','lmd','low','mid','rnd','rzd','smh','umd','unr','vwl')\r\n",
        "doublestops = ['.','?','!',':','--']\r\n",
        "singlestops = [',',';','- ']\r\n",
        "\r\n",
        "# When deciding how close two syllables are, these constants indicate how much\r\n",
        "# weight to give vowel features, stress, and consonant features\r\n",
        "kDistanceWeightVowels = 3.0\r\n",
        "kDistanceWeightStress = 3.0\r\n",
        "kDistanceWeightOnset = 2.0\r\n",
        "kDistanceWeightCoda = 5.0\r\n",
        "\r\n",
        "def phones_for_word_fb(word):\r\n",
        "  \"\"\"Phones for word - either 1st entry in the CMU pronouncing dictionary\r\n",
        "  or fallback to Pincelate sound-out if it's not in the dictionary.\r\n",
        "\r\n",
        "  Takes the first pronounciation in the dictionary if there are multiple.\r\n",
        "\r\n",
        "  Returns a list of phones.\r\n",
        "\r\n",
        "  word - lowercase word, no spaces or punctuation\r\n",
        "  \"\"\"\r\n",
        "  try:\r\n",
        "    return pronouncing.phones_for_word(word)[0].split()\r\n",
        "  except IndexError:\r\n",
        "    r = pin.soundout(word)\r\n",
        "    # Throw in an extra 'EH0' if there are no vowels in the mix already\r\n",
        "    for v, p in [(v,p) for v in vowels for p in r]:\r\n",
        "      if v in p:\r\n",
        "        return r\r\n",
        "    r.append('EH0')\r\n",
        "    return r\r\n",
        "\r\n",
        "def syllablesFromText(string):\r\n",
        "  \"\"\"Turns text into a list of Syllable objects.\r\n",
        "\r\n",
        "  One stop for ,; two stops for .:?! or double dash.\r\n",
        "  Turns a hyphen between letters (as in \"use-case\") into a space,\r\n",
        "   but in \"a phrase - like this one\" replaces it with a STOP\r\n",
        "\r\n",
        "  Discards all other punctuation and non-alpha characters (including numbers).\r\n",
        "\r\n",
        "  string -- text to convert to phones\"\"\"\r\n",
        "\r\n",
        "  lidx = ridx = 0\r\n",
        "  \r\n",
        "  syllables = []\r\n",
        "\r\n",
        "  while (lidx < len(string)):\r\n",
        "    if string[lidx].isalpha():\r\n",
        "      while ((ridx < len(string)) and (string[ridx].isalpha())):\r\n",
        "        ridx += 1\r\n",
        "      for s in syllabify.syllabify(phones_for_word_fb(string[lidx:ridx].lower())):\r\n",
        "        syllables.append(Syllable(s))\r\n",
        "      lidx = ridx\r\n",
        "    else:\r\n",
        "      while ((ridx < len(string)) and (not string[ridx].isalpha())):\r\n",
        "        ridx += 1\r\n",
        "      doublestop = False\r\n",
        "      if any(c in string[lidx:ridx] for c in doublestops):\r\n",
        "        syllables.extend([stopSyllable, stopSyllable])\r\n",
        "      else:\r\n",
        "        if any(c in string[lidx:ridx] for c in singlestops):\r\n",
        "          syllables.append(stopSyllable)\r\n",
        "      lidx = ridx\r\n",
        "\r\n",
        "  return syllables\r\n",
        "\r\n",
        "\r\n",
        "                \r\n",
        "def distance(a : Syllable, b : Syllable):\r\n",
        "  \"\"\"A distance metric between two Syllable objects.\r\n",
        "  Adjusted by the constant weights in the first cell,\r\n",
        "  applied to Manhattan distance on features.\r\n",
        "  \"\"\" \r\n",
        "  dist = abs(a.stress - b.stress) * kDistanceWeightStress\r\n",
        "\r\n",
        "  for f in consonantFeatures:\r\n",
        "    dist += abs(a.ofeat[f] - b.ofeat[f]) * kDistanceWeightOnset\r\n",
        "    dist += abs(a.cfeat[f] - b.cfeat[f]) * kDistanceWeightCoda\r\n",
        "  \r\n",
        "  for f in vowelFeatures:\r\n",
        "    dist += abs(a.vfeat[f] - b.vfeat[f]) * kDistanceWeightVowels\r\n",
        "\r\n",
        "  return dist\r\n",
        "\r\n",
        "def isVowel(phone):\r\n",
        "  return phone[0:-1] in vowels\r\n",
        "\r\n",
        "def canonFit(oldSyllables, newText, offset):\r\n",
        "  \"\"\"Given a syllable list and a batch of new text (as a string),\r\n",
        "  returns the average syllable distance of newly added syllables\r\n",
        "   to syllables *offset* syllables behind.\r\n",
        "  \r\n",
        "  Lower is rhymier.\r\n",
        "  \r\n",
        "  If newText doesn't turn up any new syllables, return a stupidly big number.\r\n",
        "\r\n",
        "  Warning: GPT-2 continuations can add partial words or just whitespace or\r\n",
        "  punctuation. This method makes no effort to re-syllabize the end of\r\n",
        "  oldSyllables or use that information to pronounce the beginning of newText.\r\n",
        "  Prepare data along word boundaries before passing to this method.\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  newSyllables = syllablesFromText(newText)\r\n",
        "\r\n",
        "  if len(newSyllables) == 0:\r\n",
        "    return float(\"inf\")\r\n",
        "  \r\n",
        "  existingLength = len(oldSyllables)\r\n",
        "  totalDist = i = averageCounter = 0\r\n",
        "\r\n",
        "  for s in newSyllables:\r\n",
        "    idx = existingLength - offset + i\r\n",
        "    # don't look back past the beginning of the text\r\n",
        "    if idx >= 0:    \r\n",
        "      if idx < existingLength:\r\n",
        "        t = oldSyllables[idx]\r\n",
        "      else:\r\n",
        "        t = newSyllables[idx - existingLength]\r\n",
        "      totalDist += distance(s, t)\r\n",
        "      averageCounter += 1\r\n",
        "    i += 1\r\n",
        "  \r\n",
        "  if averageCounter > 0:\r\n",
        "    return totalDist/averageCounter\r\n",
        "  else:\r\n",
        "    # haven't hit the canon start point, or only added STOPs that match up with STOPs;\r\n",
        "    # accept this continuation and keep going\r\n",
        "    # (theoretical risk of getting stuck in STOPland - we'll see if that's a real problem)\r\n",
        "    return 0\r\n",
        "\r\n",
        "def isascii(string):\r\n",
        "  # Apparently this is a fast way of checking because the conversion\r\n",
        "  # is implemented in C\r\n",
        "  try:\r\n",
        "    string.encode('ascii')\r\n",
        "  except UnicodeEncodeError:\r\n",
        "    return False\r\n",
        "  else:\r\n",
        "    return True"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcjQ0BKKELvx"
      },
      "source": [
        "class Continuations:\r\n",
        "  def __init__(self, alts, prompt, max_length, temperature, top_p):\r\n",
        "    self.prompt = prompt\r\n",
        "    self.max_length = max_length\r\n",
        "    self.temperature = temperature\r\n",
        "    self.top_p = top_p\r\n",
        "\r\n",
        "    self.leftToGenerate = alts\r\n",
        "    self.counter = 0\r\n",
        "    self.continuationsToTry = 256\r\n",
        "    self.cacheIter = iter([])\r\n",
        "\r\n",
        "  def __iter__(self):\r\n",
        "    return self\r\n",
        "\r\n",
        "  def __next__(self):\r\n",
        "    try:\r\n",
        "      return self.cacheIter.__next__()\r\n",
        "    except StopIteration:\r\n",
        "      if self.leftToGenerate == 0:\r\n",
        "        raise(StopIteration)\r\n",
        "        \r\n",
        "      while self.continuationsToTry > 1:\r\n",
        "        try:\r\n",
        "          n = min(self.leftToGenerate, self.continuationsToTry)\r\n",
        "          print(\"Generating \" + str(n) + \" continuations\")\r\n",
        "          self.cacheIter = iter(ai.generate(n=n,\r\n",
        "                                       prompt=self.prompt,\r\n",
        "                                       max_length=self.max_length,\r\n",
        "                                       temperature=self.temperature,\r\n",
        "                                       top_p=self.top_p,\r\n",
        "                                       return_as_list=True))\r\n",
        "          self.leftToGenerate = max(0, self.leftToGenerate - n)\r\n",
        "          return self.cacheIter.__next__()\r\n",
        "        except RuntimeError as e:\r\n",
        "          print(\"Runtime Error, reducing batch size: \" + str(e))\r\n",
        "          self.continuationsToTry = self.continuationsToTry/2\r\n",
        "      \r\n",
        "      self.cacheIter = iter(ai.generate(n=self.continuationsToTry,\r\n",
        "                                       prompt=self.prompt,\r\n",
        "                                       max_length=self.max_length,\r\n",
        "                                       temperature=self.temperature,\r\n",
        "                                       top_p=self.top_p,\r\n",
        "                                       return_as_list=True))\r\n",
        "      return self.cacheIter.__next__()\r\n",
        "\r\n",
        "\r\n",
        "def generateCanon(alts = 100, tokensPerIncrement = 5, length = 100,\r\n",
        "                  prompt = \"If only I could be a little more\",\r\n",
        "                  temperature = 1.2, top_p = 0.9, offset = 10):\r\n",
        "  \"\"\"\r\n",
        "  alts - number of samples of text to generate to test for rhymes\r\n",
        "  tokensPerIncrement - size of each incremental sample\r\n",
        "  length - total tokens (including the prompt)\r\n",
        "  prompt - start of canon\r\n",
        "  temperature, top_p - passed along to GPT-2\r\n",
        "  offset - how many syllables the 2nd part of the canon is delayed\r\n",
        "  \"\"\"\r\n",
        "  print(prompt)\r\n",
        "  canonSoFar = prompt\r\n",
        "  seed = 0\r\n",
        "  startingTokens = len(ai.tokenizer.tokenize(text=prompt))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  for i in range(startingTokens, length, tokensPerIncrement):\r\n",
        "    # Tricky thing - GPT-2's tokens are sometimes non-words and partial words.\r\n",
        "    # \"don\" might end a previous pass and be expanded to \"don't\" in a continuation.\r\n",
        "    # Or a continuation could add only line breaks and punctuation.    \r\n",
        "    # So we split canonSoFar up to the last non-alpha character.\r\n",
        "    # We'll attach the rest of canonSoFar to the newly generated text.\r\n",
        "    lastNonAlphaInCanonSoFar = len(canonSoFar) - 1\r\n",
        "    while lastNonAlphaInCanonSoFar > 0:\r\n",
        "      if not canonSoFar[lastNonAlphaInCanonSoFar].isalpha():\r\n",
        "        break\r\n",
        "      lastNonAlphaInCanonSoFar = lastNonAlphaInCanonSoFar - 1\r\n",
        "    \r\n",
        "    solidCanon = canonSoFar[0:lastNonAlphaInCanonSoFar]\r\n",
        "    tentativeCanon = canonSoFar[lastNonAlphaInCanonSoFar:]\r\n",
        "    \r\n",
        "    solidCanonSyllables = syllablesFromText(solidCanon)  \r\n",
        "    \r\n",
        "    bestTrial = canonSoFar\r\n",
        "    bestDist = float(\"inf\")\r\n",
        "\r\n",
        "    continuations = Continuations(alts, canonSoFar, i, temperature,\r\n",
        "                                  top_p)\r\n",
        "    for trial in continuations:\r\n",
        "      newText = tentativeCanon + trial[len(canonSoFar):]\r\n",
        "      if (isascii(newText)  # only test generated text that our pronounciation lookups can handle\r\n",
        "          and not any(c.isdigit() for c in newText)):  # discard text with numbers \r\n",
        "        trialFit = canonFit(solidCanonSyllables, newText, offset)\r\n",
        "        if trialFit < bestDist and trialFit > 0: # discourage exact loops\r\n",
        "          bestDist = trialFit\r\n",
        "          bestTrial = trial\r\n",
        "    \r\n",
        "    canonSoFar = bestTrial\r\n",
        "    print()\r\n",
        "    print(canonSoFar)\r\n",
        "\r\n",
        "  print()\r\n",
        "  print(\"FINAL CANON:\")\r\n",
        "  print(canonSoFar)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VMJq4r23Iolk",
        "outputId": "b19e6825-b969-40c0-9d87-cc673a10122e"
      },
      "source": [
        "generateCanon(alts = 250, tokensPerIncrement=2, length=200, prompt=\"A transition matrix\", offset = 10, temperature = .5, top_p = 0.9)\r\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A transition matrix\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes,\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-c\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous,\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is:\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Do\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            "\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book,\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book,\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with just\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for the wrong\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for the wrongness of\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for the wrongness of others.\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for the wrongness of others.\n",
            "And\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for the wrongness of others.\n",
            "And parenting isn\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for the wrongness of others.\n",
            "And parenting isn't all\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for the wrongness of others.\n",
            "And parenting isn't all about correcting\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for the wrongness of others.\n",
            "And parenting isn't all about correcting behaviors;\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for the wrongness of others.\n",
            "And parenting isn't all about correcting behaviors; it's\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for the wrongness of others.\n",
            "And parenting isn't all about correcting behaviors; it's about telling\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for the wrongness of others.\n",
            "And parenting isn't all about correcting behaviors; it's about telling the truth\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for the wrongness of others.\n",
            "And parenting isn't all about correcting behaviors; it's about telling the truth about yourself\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for the wrongness of others.\n",
            "And parenting isn't all about correcting behaviors; it's about telling the truth about yourself - that\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for the wrongness of others.\n",
            "And parenting isn't all about correcting behaviors; it's about telling the truth about yourself - that's the\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for the wrongness of others.\n",
            "And parenting isn't all about correcting behaviors; it's about telling the truth about yourself - that's the moral agency\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix.\n",
            "The power core of the brain is composed of volunteer genes, encodes a set of protein-coding genes, and a set of RNA-ed genes.\n",
            "When we look at a given scene, we can see that the amount of variation is not random, but continuous, like when we look at a map.\n",
            "The general idea is: justify.\n",
            "Doing the right thing will do the right thing.\n",
            "Sometimes the right thing is just the result of good parenting.\n",
            " parenting exercises.\n",
            "You've written a great parenting book, but people aren't coming back - what you've written is really just another parenting book, with justifications for the wrongness of others.\n",
            "And parenting isn't all about correcting behaviors; it's about telling the truth about yourself - that's the moral agency.\n",
            "\n",
            "Generating 250 continuations\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ed680cfac3f1>\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcacheIter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ed680cfac3f1>\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m                                        \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                                        return_as_list=True))\n\u001b[0m\u001b[1;32m     33\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleftToGenerate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleftToGenerate\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/aitextgen/aitextgen.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, n, prompt, max_length, temperature, do_sample, return_as_list, seed, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_specific_kwargs)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_generate_no_beam_search\u001b[0;34m(self, input_ids, cur_len, max_length, min_length, do_sample, temperature, top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, decoder_start_token_id, batch_size, encoder_outputs, attention_mask, use_cache, model_specific_kwargs)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache)\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache)\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, use_cache)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, use_cache)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mpresent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# transpose to have same shapes for stacking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 15.90 GiB total capacity; 14.31 GiB already allocated; 217.88 MiB free; 14.73 GiB reserved in total by PyTorch)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-28245f32ce29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerateCanon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokensPerIncrement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"A transition matrix\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-ed680cfac3f1>\u001b[0m in \u001b[0;36mgenerateCanon\u001b[0;34m(alts, tokensPerIncrement, length, prompt, temperature, top_p, offset)\u001b[0m\n\u001b[1;32m     86\u001b[0m     continuations = Continuations(alts, canonSoFar, i, temperature,\n\u001b[1;32m     87\u001b[0m                                   top_p)\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontinuations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0mnewText\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtentativeCanon\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanonSoFar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       if (isascii(newText)  # only test generated text that our pronounciation lookups can handle\n",
            "\u001b[0;32m<ipython-input-5-ed680cfac3f1>\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcacheIter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Runtime Error, reducing batch size: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinuationsToTry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinuationsToTry\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: must be str, not RuntimeError"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXmPunR8-E2w"
      },
      "source": [
        "with new syllabification, temperature 5, top_p .2, tokensPerIncrement = 5, offset 5, params 4/5/2/3 (vowels/stress/onset/coda)\r\n",
        "\r\n",
        "```\r\n",
        "If only I could be in a meeting coordinating this, explaining things.\r\n",
        "Getting these people to believe I can't deliver on time has never made any difference, even in the event that they're irate and cry when I'm explaining why we didn't, I haven't, I haven't, it never was; it never will, it never will; if I were building this case, building this case, building a case about me and [1000Plateaus] I probably would have had a clearer shot at what people thought I was doing than I was.\r\n",
        "I am as bad as I think myself as I can be when it happens to me - I can't be in an arty tent in an alleyway in an arty prison, hardly eating, hardly paying my tab, staring at strangers ferried here from the slobbery of the north, a dozen screaming asylum-climbers and clinging desperately to their property - I have plenty of time, money\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WH8dvGm2miz"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "If  only I  could be in that  boat.       That boat.\r\n",
        "            If    only  I     could be in that boat.\r\n",
        "\r\n",
        "I    don't know why.       Cousinwog, cousin of  Brassy and\r\n",
        "That boat.         I don't know why.  Cousinwog,     cousin \r\n",
        "\r\n",
        "Red from    Cousinwog  and   Mighty from   Cousinwog  \r\n",
        "of  Brassy     and Red from  Cousinwog    and Mighty from\r\n",
        "\r\n",
        "City  and  Plateaus  aren't  together        anymore.   They're\r\n",
        "Cousinwog  City      and     Plateaus aren't together anymore. \r\n",
        "\r\n",
        "having a breakout and Plateaus says \"What's going    on    here?\"\r\n",
        "      They're having   a breakout    and    Plateaus says \"What's\r\n",
        "\r\n",
        "going on here?\"\r\n",
        "```\r\n",
        "\r\n",
        "(offset of 4, temperature .7)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw7ljx8z7Gid"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "If only I  could be my good   editor     in one piece \r\n",
        "        If only     I  could  be my good editor\r\n",
        "\r\n",
        "-- which, at least,   it   was, when  absolutely forced and wri-\r\n",
        "editor   --  which at least,    it   was, when absolutely\r\n",
        "\r\n",
        "ting   ridiculously    long chunks of crap chunks.\r\n",
        "forced and writing ridiculously       long chunks of crap\r\n",
        "\r\n",
        "Without the   ethics.          \"It sucks.\"    Sometimes,\r\n",
        "chunks.       Without  the  ethics.       \"It sucks.\"\r\n",
        "\r\n",
        "sometimes, it's just \"it's difficult       to tell a\r\n",
        "Sometimes, sometimes,      it's just \"it's difficult\r\n",
        "\r\n",
        "story\"   as art,   or an illusion.    There's another,     ever\r\n",
        "to tell  a  story\" as art, or an illusion.       There's another,\r\n",
        "\r\n",
        "another trance.  Sometimes it's \"I don't  think it's\r\n",
        "   ever another  trance.        Sometimes it's  \"I\r\n",
        "\r\n",
        "important\"         as   entertainment.\r\n",
        "don't think it's   important\" as  entertainment.\r\n",
        "```\r\n",
        "(offset of 3, temperature 2.0, vowels 4, stress 3, consonants 5)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGcVE1FaA4hj"
      },
      "source": [
        "```\r\n",
        "If only I could be allowed to  be loved.     Been  avoiding\r\n",
        "          If    only  I  could be allowed to be    loved.\r\n",
        "\r\n",
        "eye  doctor since  ninety-two.\r\n",
        "Been avoiding      eye doctor  since ninety-two.\r\n",
        "```\r\n",
        "\r\n",
        "(offset of 4, temperature 2, vowels, stress, cons = 2,1,2; 3 tokens per increment)\r\n",
        "\r\n",
        "```\r\n",
        "If only I could be that good.\r\n",
        "People say it - \"It's true it isn't\" - \"It is\", \"It is true it is true\" - \"Dear @lukejchengnott, in what way?\"\r\n",
        "```\r\n",
        "\r\n",
        "(same settings, but offset of 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAhT-dl3H8oB"
      },
      "source": [
        "```\r\n",
        "If only\r\n",
        "I could be\r\n",
        "as good as\r\n",
        "this.\r\n",
        "I know I'm\r\n",
        "doing ev\r\n",
        "ery thing right.\r\n",
        "          I\r\n",
        "know I'm do\r\n",
        "ing every\r\n",
        "thing I can.\r\n",
        "          I\r\n",
        "know I'm do\r\n",
        "ing every\r\n",
        "thing I can.\r\n",
        "          I\r\n",
        "know I'm cap\r\n",
        "able.\r\n",
        "     I be\r\n",
        "lieve I am\r\n",
        "loved.\r\n",
        "I believe\r\n",
        "I am ent\r\n",
        "ertaining\r\n",
        "the impos\r\n",
        "sible.\r\n",
        "  But all\r\n",
        "of a   sud\r\n",
        "den it seems\r\n",
        "like it's a\r\n",
        "bout to col\r\n",
        "lapse on it\r\n",
        "self and I\r\n",
        "have to find\r\n",
        "a place to\r\n",
        "stand,  so\r\n",
        "I don't lis\r\n",
        "ten.\r\n",
        "Finally\r\n",
        "I find a\r\n",
        "place to sit,\r\n",
        "   thinking\r\n",
        "it's okay\r\n",
        "to be there.\r\n",
        "      There\r\n",
        "is a rea\r\n",
        "sonable\r\n",
        "place to go,\r\n",
        "    but I'm\r\n",
        "worried it\r\n",
        "will be rai\r\n",
        "ded by the\r\n",
        "big demon\r\n",
        "ic fuckups,\r\n",
        "   and killed.\r\n",
        "          I\r\n",
        "know that I\r\n",
        "am allowed\r\n",
        "to feel my\r\n",
        "anger and\r\n",
        "anxie\r\n",
        "ty under\r\n",
        "control,\r\n",
        "that this is\r\n",
        "an extreme\r\n",
        "ly danger\r\n",
        "ous place.\r\n",
        "    I sit\r\n",
        "on it,\r\n",
        "thinking it's\r\n",
        "really o\r\n",
        "kay to be\r\n",
        "there.\r\n",
        "But then I\r\n",
        "get really\r\n",
        "worried and\r\n",
        "angry   and\r\n",
        "a     pile  of\r\n",
        "Trash piles up.\r\n",
        "```\r\n",
        "\r\n",
        "(alts -> 50 from 250, 5 tokens per increment, temperature 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77lE-sKzKLGt"
      },
      "source": [
        "(stress weight 1->200, offset -> 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ohIoeqXahkw",
        "outputId": "a72fc1ab-6b39-47f5-c87d-efac20e8d2c1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 363
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOWldxrsqVjh"
      },
      "source": [
        "kDistanceWeightVowels = 3.0\r\n",
        "kDistanceWeightStress = 3.0\r\n",
        "kDistanceWeightOnset = 2.0\r\n",
        "kDistanceWeightCoda = 5.0\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iulVyEdBa-Wr",
        "outputId": "eb0ac510-0ef5-4c65-e4b5-c62e9ea5b422"
      },
      "source": [
        "syllabify.syllabify(phones_for_word_fb(\"transmute\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['T', 'R'], ['AE0'], ['N', 'S']), (['M'], ['Y', 'UW1'], ['T'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2VxdaZ8bHZC"
      },
      "source": [
        "kDistanceWeightVowels = 3.0\r\n",
        "kDistanceWeightStress = 3.0\r\n",
        "kDistanceWeightOnset = 2.0\r\n",
        "kDistanceWeightCoda = 5.0\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBy6nfHgv7rt",
        "outputId": "d4e70ac7-711a-4e14-f3ed-f3ed3440c015"
      },
      "source": [
        "kDistanceWeightCoda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}