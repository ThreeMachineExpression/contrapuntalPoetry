{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PhoneticSimilarity.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMvT8UnmvKlbx/EaiLN4NCN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThreeMachineExpression/contrapuntalPoetry/blob/main/PhoneticSimilarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPCyH2eLsbbF"
      },
      "source": [
        "\r\n",
        "Parts of this notebook are based on Max Woolf's aitextgen notebook, as updated by Allison Parrish.\r\n",
        "\r\n",
        "Also makes use of Kyle Gorman's syllabification library, syllabify.\r\n",
        "\r\n",
        "Initial implementation looked for slant rhyme / slant alliteration by grabbing all of the consonants from the boundaries between vowels and summing their features, but that leads to too-mushy results. The syllabification approach misses assonance that crosses syllable boundaries - a todo is to figure out how to bring some of that back.\r\n",
        "\r\n",
        "Todo: correctly handle single letters (as in e.g.). It sounds them out correctly, but adds two STOPs between each for the periods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxzLNBUjsxSv"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYABCBZZs2ie",
        "outputId": "d0c2b273-8be4-4e85-e380-c1f5ae354a3a"
      },
      "source": [
        "# Freeze versions of dependencies for now\r\n",
        "!pip install tensorflow==1.15.0 keras==2.2.5 \"h5py<3.0.0\"\r\n",
        "!pip3 install pytorch-lightning==0.7.6\r\n",
        "!pip3 install transformers==2.9.1\r\n",
        "!pip3 install fire==0.3.0\r\n",
        "\r\n",
        "!pip install -q aitextgen==0.2.3\r\n",
        "\r\n",
        "from aitextgen import aitextgen\r\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive\r\n",
        "\r\n",
        "mount_gdrive()\r\n",
        "\r\n",
        "!pip install annoy\r\n",
        "!pip install pronouncing\r\n",
        "!pip install pincelate\r\n",
        "import pronouncing\r\n",
        "import pincelate\r\n",
        "pin = pincelate.Pincelate()\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "import random\r\n",
        "import textwrap\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import logging\r\n",
        "logging.basicConfig(\r\n",
        "        format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\r\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n",
        "        level=logging.INFO\r\n",
        "    )\r\n",
        "\r\n",
        "kPhoneticSimilarityVectorsRepo = \"https://github.com/aparrish/phonetic-similarity-vectors\"\r\n",
        "\r\n",
        "!git clone {kPhoneticSimilarityVectorsRepo}\r\n",
        "\r\n",
        "!python -m spacy download en_core_web_md\r\n",
        "\r\n",
        "%cd phonetic-similarity-vectors\r\n",
        "from featurephone import phone_feature_map as pfm\r\n",
        "%cd ..\r\n",
        "\r\n",
        "kSyllabifyRepo = \"https://github.com/threemachineexpression/syllabify\"\r\n",
        "!git clone {kSyllabifyRepo}\r\n",
        "\r\n",
        "%cd syllabify\r\n",
        "import syllabify\r\n",
        "%cd .."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 28kB/s \n",
            "\u001b[?25hCollecting keras==2.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/ba/2d058dcf1b85b9c212cc58264c98a4a7dd92c989b798823cc5690d062bb2/Keras-2.2.5-py2.py3-none-any.whl (336kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 44.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py<3.0.0 in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.36.2)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 49.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.19.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0) (51.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=91498d9cc11a3684b59fce5857da7fcd550f06c4475b4079fba10ef7d060ae25\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, gast, tensorflow-estimator, tensorboard, tensorflow, keras\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.0\n",
            "    Uninstalling tensorboard-2.4.0:\n",
            "      Successfully uninstalled tensorboard-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.0\n",
            "    Uninstalling tensorflow-2.4.0:\n",
            "      Successfully uninstalled tensorflow-2.4.0\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed gast-0.2.2 keras-2.2.5 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n",
            "Collecting pytorch-lightning==0.7.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/ab/561d1fa6e5af30b2fd7cb4001f93eb08531e1b72976f13eebf7f7cdc021c/pytorch_lightning-0.7.6-py3-none-any.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.6) (4.41.1)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.6) (1.15.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.6) (1.7.0+cu101)\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.6) (3.13)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.6) (1.19.5)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (0.36.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (0.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (51.3.3)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (1.32.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (3.3.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.6) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch-lightning==0.7.6) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch-lightning==0.7.6) (0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.7.6) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning==0.7.6) (3.4.0)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=7f158d2cb668f2fa9ecd10253fab4e53a166359932a3f0e560922f01cf016a00\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built future\n",
            "Installing collected packages: future, pytorch-lightning\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed future-0.18.2 pytorch-lightning-0.7.6\n",
            "Collecting transformers==2.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 4.3MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 20.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 49.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.1) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.1) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.1) (2020.12.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=fef841488c38fda3a42897e6db76c5bbbbdf66be403cb837ac32df98dad4bf8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.7.0 transformers-2.9.1\n",
            "Collecting fire==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/42/6252075cbad90b9efb27b6586827779758c62fd73868a1cd0d23ebb5aac6/fire-0.3.0.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire==0.3.0) (1.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.3.0-py2.py3-none-any.whl size=111108 sha256=da216c60dd1cef16fd1764e2158d3cf6fd1c372f4e5007c31417972a5c11933e\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/62/79/6a40acd827ec9d78d610be311820ecf8e41db024d8b1d12ace\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.3.0\n",
            "\u001b[K     |████████████████████████████████| 573kB 4.0MB/s \n",
            "\u001b[?25h  Building wheel for aitextgen (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Mounted at /content/drive\n",
            "Collecting annoy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/5b/1c22129f608b3f438713b91cd880dc681d747a860afe3e8e0af86e921942/annoy-1.17.0.tar.gz (646kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 6.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp36-cp36m-linux_x86_64.whl size=390347 sha256=f4d526b912c8dfe74de07da02874076ac82cabbcade452123fcec32540e1eb29\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/c5/59/cce7e67b52c8e987389e53f917b6bb2a9d904a03246fadcb1e\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n",
            "Collecting pronouncing\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/c6/9dc74a3ddca71c492e224116b6654592bfe5717b4a78582e4d9c3345d153/pronouncing-0.2.0.tar.gz\n",
            "Collecting cmudict>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/77/f009abf803286876fa99cb7bd9d1132c7b64a0b34a0360666275ce1bc733/cmudict-0.4.5-py2.py3-none-any.whl (939kB)\n",
            "\u001b[K     |████████████████████████████████| 942kB 5.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pronouncing\n",
            "  Building wheel for pronouncing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pronouncing: filename=pronouncing-0.2.0-py2.py3-none-any.whl size=6223 sha256=55a7d192377702a8214b166f0ffe24ddd557e9b50e97611384a6c3e0a700ac2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/fd/e8/fb1a226f707c7e20dbed4c43f81b819d279ffd3b0e2f06ee13\n",
            "Successfully built pronouncing\n",
            "Installing collected packages: cmudict, pronouncing\n",
            "Successfully installed cmudict-0.4.5 pronouncing-0.2.0\n",
            "Collecting pincelate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/b0/b8c6101e3e643cd712f42e8a5b4fb01dd5c3255c975f66d6673386341caa/pincelate-0.0.1-py3-none-any.whl (12.0MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0MB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from pincelate) (0.22.2.post1)\n",
            "Requirement already satisfied: Keras>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pincelate) (2.2.5)\n",
            "Requirement already satisfied: pronouncing>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from pincelate) (0.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->pincelate) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->pincelate) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->pincelate) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.0->pincelate) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.0->pincelate) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.0->pincelate) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.0->pincelate) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.0->pincelate) (3.13)\n",
            "Requirement already satisfied: cmudict>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from pronouncing>=0.2.0->pincelate) (0.4.5)\n",
            "Installing collected packages: pincelate\n",
            "Successfully installed pincelate-0.0.1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'phonetic-similarity-vectors'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Total 32 (delta 0), reused 0 (delta 0), pack-reused 32\u001b[K\n",
            "Unpacking objects: 100% (32/32), done.\n",
            "Collecting en_core_web_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp36-none-any.whl size=98051304 sha256=2ca5ad03e7057ed1701147eff9cc324b8ccce1743514e62345db2878f5be0969\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tuf9uc4_/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "/content/phonetic-similarity-vectors\n",
            "/content\n",
            "Cloning into 'syllabify'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 62 (delta 3), reused 0 (delta 0), pack-reused 53\u001b[K\n",
            "Unpacking objects: 100% (62/62), done.\n",
            "/content/syllabify\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZkOeQ8V4zhX"
      },
      "source": [
        "from_folder = \"aitextgenDreamFineTuning\"\r\n",
        "\r\n",
        "for file in [\"pytorch_model.bin\", \"config.json\"]:\r\n",
        "  if from_folder:\r\n",
        "    copy_file_from_gdrive(file, from_folder)\r\n",
        "  else:\r\n",
        "    copy_file_from_gdrive(file)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9afxhkk46WG",
        "outputId": "172fc931-c588-4043-ad75-4ba619409f94"
      },
      "source": [
        "ai = aitextgen(model=\"pytorch_model.bin\", config=\"config.json\", to_gpu=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:aitextgen:Loading GPT-2 model from provided pytorch_model.bin.\n",
            "INFO:aitextgen:Using the default GPT-2 Tokenizer.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXb_7Rin5hr-"
      },
      "source": [
        "# Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DdUOTYE5tad",
        "outputId": "ef7ed54f-7294-45a9-ad3d-2675d214b53d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jan 24 02:49:38 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    40W / 300W |  16087MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H4_V452Dy5u",
        "outputId": "53cbae9b-445f-4de4-dc82-d391c2593a70"
      },
      "source": [
        "ai.generate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "You've made my night so much easier.\n",
            "Oh, I'm so sorry.\n",
            "I'm so sorry.\n",
            "I'm so sorry.\n",
            "I know you didn't mean it, but I know you didn't.\n",
            "I was just so sorry I couldn't get out from under you.\n",
            "And I could be kicked out of Eastman for it, but really I could be kicked out for anything, I'm already sneezing the wrong way, I'm starving, and having a really bad case of scabies, and I wake up the next morning thinking I've been impregnated, and it wouldn't have happened if I hadn't had the thought that what you were thinking.\n",
            "So I'm trying really hard to think of what I'm going to say to get out of this, to make sure no one sees it, to make sure that no one sees my weaknesses, to make sure that the people who love me don't hurt me when I say it, to make sure that the damage isn't irreparable.\n",
            "I'm trying really hard to pretend that I don't know anything about this kid, that I know how special this kid is and how talented this kid is, that I don't have to see his face in real\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftsVytZcEGLj"
      },
      "source": [
        "# Phonetic Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVnfSUgmEqNd"
      },
      "source": [
        "# Strategy:\r\n",
        "# Split text into syllable buckets\r\n",
        "# In each bucket, include the vowel sound, stress, and all of the bordering phonemes\r\n",
        "#  (so a consonant between syllables goes in both buckets)\r\n",
        "# Measure the distance between two buckets as \r\n",
        "#   a * (manhattan distance of consonant features)\r\n",
        "# + b * (manhattan distance of vowel features)\r\n",
        "# + c * (stress distance)\r\n",
        "# (with a, b, c tuned as desired to emphasize different similarities)\r\n",
        "#\r\n",
        "# Pauses get a bucket of their own.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class Syllable:\r\n",
        "  # initialization takes a list of 3 lists of strings:\r\n",
        "  # the phones for onset, nucleus, and coda\r\n",
        "  # (as per the output of syllabify).\r\n",
        "  #\r\n",
        "  # vowel - vowel phone (w/o stress indicator) or STOP\r\n",
        "  # stress - 0 for STOP, 1 for unstressed, 2 2ndary stress, 3 primary stress\r\n",
        "  # (note this is different from the arpabet numbers)\r\n",
        "  # ofeat - Counter containing a count of the consonant features in the onset\r\n",
        "  # cfeat - Counter containing a count of the consonant features in the coda\r\n",
        "  # vfeat - Counter containing a count of the vowel features\r\n",
        "  def __init__(self, syllableList):\r\n",
        "    self.vowel = syllableList[1][0][0:-1]\r\n",
        "\r\n",
        "    # convert arpabet stress to a representation capable of similarity math\r\n",
        "    stressConversion = {\r\n",
        "        '0': 1,\r\n",
        "        '1': 3,\r\n",
        "        '2': 2\r\n",
        "    }\r\n",
        "\r\n",
        "    if self.vowel == 'STOP':\r\n",
        "      self.stress = 0\r\n",
        "    else:\r\n",
        "      self.stress = stressConversion[syllableList[1][0][-1]]\r\n",
        "      \r\n",
        "    self.ofeat = Counter()\r\n",
        "    self.cfeat = Counter()\r\n",
        "    self.vfeat = Counter()\r\n",
        "\r\n",
        "    for feature in pfm[self.vowel]:\r\n",
        "      self.vfeat[feature] += 1\r\n",
        "    \r\n",
        "    for phone in syllableList[0]:\r\n",
        "      for feature in pfm[phone]:\r\n",
        "        self.ofeat[feature] += 1\r\n",
        "    \r\n",
        "    for phone in syllableList[2]:\r\n",
        "      for feature in pfm[phone]:\r\n",
        "        self.cfeat[feature] += 1\r\n",
        "\r\n",
        "pfm['STOP'] = ()\r\n",
        "stopSyllable = Syllable(([],['STOP0'],[]))\r\n",
        "\r\n",
        "vowels = ('AO','AA','IY','UW','EH','IH','UH','AH','AE','EY','AY','OW','AW','OY',\r\n",
        "          'ER','STOP')\r\n",
        "consonantFeatures = ('alv','apr','asp','blb','dnt','frc','glt','lat','lbd','lbv','nas','pal','pla','stp','vcd','vel','vls')\r\n",
        "vowelFeatures = ('bck','cnt','fnt','hgh','lmd','low','mid','rnd','rzd','smh','umd','unr','vwl')\r\n",
        "doublestops = ['.','?','!',':','--']\r\n",
        "singlestops = [',',';','- ']\r\n",
        "\r\n",
        "# When deciding how close two syllables are, these constants indicate how much\r\n",
        "# weight to give vowel features, stress, and consonant features\r\n",
        "kDistanceWeightVowels = 3.0\r\n",
        "kDistanceWeightStress = 3.0\r\n",
        "kDistanceWeightOnset = 2.0\r\n",
        "kDistanceWeightCoda = 5.0\r\n",
        "\r\n",
        "def phones_for_word_fb(word):\r\n",
        "  \"\"\"Phones for word - either 1st entry in the CMU pronouncing dictionary\r\n",
        "  or fallback to Pincelate sound-out if it's not in the dictionary.\r\n",
        "\r\n",
        "  Takes the first pronounciation in the dictionary if there are multiple.\r\n",
        "\r\n",
        "  Returns a list of phones.\r\n",
        "\r\n",
        "  word - lowercase word, no spaces or punctuation\r\n",
        "  \"\"\"\r\n",
        "  try:\r\n",
        "    return pronouncing.phones_for_word(word)[0].split()\r\n",
        "  except IndexError:\r\n",
        "    r = pin.soundout(word)\r\n",
        "    # Throw in an extra 'EH0' if there are no vowels in the mix already\r\n",
        "    for v, p in [(v,p) for v in vowels for p in r]:\r\n",
        "      if v in p:\r\n",
        "        return r\r\n",
        "    r.append('EH0')\r\n",
        "    return r\r\n",
        "\r\n",
        "def syllablesFromText(string):\r\n",
        "  \"\"\"Turns text into a list of Syllable objects.\r\n",
        "\r\n",
        "  One stop for ,; two stops for .:?! or double dash.\r\n",
        "  Turns a hyphen between letters (as in \"use-case\") into a space,\r\n",
        "   but in \"a phrase - like this one\" replaces it with a STOP\r\n",
        "\r\n",
        "  Discards all other punctuation and non-alpha characters (including numbers).\r\n",
        "\r\n",
        "  string -- text to convert to phones\"\"\"\r\n",
        "\r\n",
        "  lidx = ridx = 0\r\n",
        "  \r\n",
        "  syllables = []\r\n",
        "\r\n",
        "  while (lidx < len(string)):\r\n",
        "    if string[lidx].isalpha():\r\n",
        "      while ((ridx < len(string)) and (string[ridx].isalpha())):\r\n",
        "        ridx += 1\r\n",
        "      for s in syllabify.syllabify(phones_for_word_fb(string[lidx:ridx].lower())):\r\n",
        "        syllables.append(Syllable(s))\r\n",
        "      lidx = ridx\r\n",
        "    else:\r\n",
        "      while ((ridx < len(string)) and (not string[ridx].isalpha())):\r\n",
        "        ridx += 1\r\n",
        "      doublestop = False\r\n",
        "      if any(c in string[lidx:ridx] for c in doublestops):\r\n",
        "        syllables.extend([stopSyllable, stopSyllable])\r\n",
        "      else:\r\n",
        "        if any(c in string[lidx:ridx] for c in singlestops):\r\n",
        "          syllables.append(stopSyllable)\r\n",
        "      lidx = ridx\r\n",
        "\r\n",
        "  return syllables\r\n",
        "\r\n",
        "\r\n",
        "                \r\n",
        "def distance(a : Syllable, b : Syllable):\r\n",
        "  \"\"\"A distance metric between two Syllable objects.\r\n",
        "  Adjusted by the constant weights in the first cell,\r\n",
        "  applied to Manhattan distance on features.\r\n",
        "  \"\"\" \r\n",
        "  dist = abs(a.stress - b.stress) * kDistanceWeightStress\r\n",
        "\r\n",
        "  for f in consonantFeatures:\r\n",
        "    dist += abs(a.ofeat[f] - b.ofeat[f]) * kDistanceWeightOnset\r\n",
        "    dist += abs(a.cfeat[f] - b.cfeat[f]) * kDistanceWeightCoda\r\n",
        "  \r\n",
        "  for f in vowelFeatures:\r\n",
        "    dist += abs(a.vfeat[f] - b.vfeat[f]) * kDistanceWeightVowels\r\n",
        "\r\n",
        "  return dist\r\n",
        "\r\n",
        "def isVowel(phone):\r\n",
        "  return phone[0:-1] in vowels\r\n",
        "\r\n",
        "def canonFit(oldSyllables, newText, offset):\r\n",
        "  \"\"\"Given a syllable list and a batch of new text (as a string),\r\n",
        "  returns the average syllable distance of newly added syllables\r\n",
        "   to syllables *offset* syllables behind.\r\n",
        "  \r\n",
        "  Lower is rhymier.\r\n",
        "  \r\n",
        "  If newText doesn't turn up any new syllables, return a stupidly big number.\r\n",
        "\r\n",
        "  Warning: GPT-2 continuations can add partial words or just whitespace or\r\n",
        "  punctuation. This method makes no effort to re-syllabize the end of\r\n",
        "  oldSyllables or use that information to pronounce the beginning of newText.\r\n",
        "  Prepare data along word boundaries before passing to this method.\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  newSyllables = syllablesFromText(newText)\r\n",
        "\r\n",
        "  if len(newSyllables) == 0:\r\n",
        "    return float(\"inf\")\r\n",
        "  \r\n",
        "  existingLength = len(oldSyllables)\r\n",
        "  totalDist = i = averageCounter = 0\r\n",
        "\r\n",
        "  for s in newSyllables:\r\n",
        "    idx = existingLength - offset + i\r\n",
        "    # don't look back past the beginning of the text\r\n",
        "    if idx >= 0:    \r\n",
        "      if idx < existingLength:\r\n",
        "        t = oldSyllables[idx]\r\n",
        "      else:\r\n",
        "        t = newSyllables[idx - existingLength]\r\n",
        "      totalDist += distance(s, t)\r\n",
        "      averageCounter += 1\r\n",
        "    i += 1\r\n",
        "  \r\n",
        "  if averageCounter > 0:\r\n",
        "    return totalDist/averageCounter\r\n",
        "  else:\r\n",
        "    # haven't hit the canon start point, or only added STOPs that match up with STOPs;\r\n",
        "    # accept this continuation and keep going\r\n",
        "    # (theoretical risk of getting stuck in STOPland - we'll see if that's a real problem)\r\n",
        "    return 0\r\n",
        "\r\n",
        "def isascii(string):\r\n",
        "  # Apparently this is a fast way of checking because the conversion\r\n",
        "  # is implemented in C\r\n",
        "  try:\r\n",
        "    string.encode('ascii')\r\n",
        "  except UnicodeEncodeError:\r\n",
        "    return False\r\n",
        "  else:\r\n",
        "    return True"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcjQ0BKKELvx"
      },
      "source": [
        "class Continuations:\r\n",
        "  def __init__(self, alts, prompt, max_length, temperature, top_p):\r\n",
        "    self.prompt = prompt\r\n",
        "    self.max_length = max_length\r\n",
        "    self.temperature = temperature\r\n",
        "    self.top_p = top_p\r\n",
        "\r\n",
        "    self.leftToGenerate = alts\r\n",
        "    self.counter = 0\r\n",
        "    self.continuationsToTry = 256\r\n",
        "    self.cacheIter = iter([])\r\n",
        "\r\n",
        "  def __iter__(self):\r\n",
        "    return self\r\n",
        "\r\n",
        "  def __next__(self):\r\n",
        "    try:\r\n",
        "      return self.cacheIter.__next__()\r\n",
        "    except StopIteration:\r\n",
        "      if self.leftToGenerate == 0:\r\n",
        "        raise(StopIteration)\r\n",
        "        \r\n",
        "      while self.continuationsToTry > 1:\r\n",
        "        try:\r\n",
        "          n = min(self.leftToGenerate, self.continuationsToTry)\r\n",
        "          self.cacheIter = iter(ai.generate(n=n,\r\n",
        "                                       prompt=self.prompt,\r\n",
        "                                       max_length=self.max_length,\r\n",
        "                                       temperature=self.temperature,\r\n",
        "                                       top_p=self.top_p,\r\n",
        "                                       return_as_list=True))\r\n",
        "          self.leftToGenerate = max(0, self.leftToGenerate - n)\r\n",
        "          return self.cacheIter.__next__()\r\n",
        "        except RuntimeError as e:\r\n",
        "          print(\"Runtime Error, reducing batch size: \" + str(e))\r\n",
        "          self.continuationsToTry = self.continuationsToTry//2\r\n",
        "      \r\n",
        "      self.cacheIter = iter(ai.generate(n=self.continuationsToTry,\r\n",
        "                                       prompt=self.prompt,\r\n",
        "                                       max_length=self.max_length,\r\n",
        "                                       temperature=self.temperature,\r\n",
        "                                       top_p=self.top_p,\r\n",
        "                                       return_as_list=True))\r\n",
        "      return self.cacheIter.__next__()\r\n",
        "\r\n",
        "\r\n",
        "def generateCanon(alts = 100, tokensPerIncrement = 5, length = 100,\r\n",
        "                  prompt = \"If only I could be a little more\",\r\n",
        "                  temperature = 1.2, top_p = 0.9, offset = 10):\r\n",
        "  \"\"\"\r\n",
        "  alts - number of samples of text to generate to test for rhymes\r\n",
        "  tokensPerIncrement - size of each incremental sample\r\n",
        "  length - total tokens (including the prompt)\r\n",
        "  prompt - start of canon\r\n",
        "  temperature, top_p - passed along to GPT-2\r\n",
        "  offset - how many syllables the 2nd part of the canon is delayed\r\n",
        "  \"\"\"\r\n",
        "  print(prompt)\r\n",
        "  canonSoFar = prompt\r\n",
        "  seed = 0\r\n",
        "  startingTokens = len(ai.tokenizer.tokenize(text=prompt))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  for i in range(startingTokens, length, tokensPerIncrement):\r\n",
        "    # Tricky thing - GPT-2's tokens are sometimes non-words and partial words.\r\n",
        "    # \"don\" might end a previous pass and be expanded to \"don't\" in a continuation.\r\n",
        "    # Or a continuation could add only line breaks and punctuation.    \r\n",
        "    # So we split canonSoFar up to the last non-alpha character.\r\n",
        "    # We'll attach the rest of canonSoFar to the newly generated text.\r\n",
        "    lastNonAlphaInCanonSoFar = len(canonSoFar) - 1\r\n",
        "    while lastNonAlphaInCanonSoFar > 0:\r\n",
        "      if not canonSoFar[lastNonAlphaInCanonSoFar].isalpha():\r\n",
        "        break\r\n",
        "      lastNonAlphaInCanonSoFar = lastNonAlphaInCanonSoFar - 1\r\n",
        "    \r\n",
        "    solidCanon = canonSoFar[0:lastNonAlphaInCanonSoFar]\r\n",
        "    tentativeCanon = canonSoFar[lastNonAlphaInCanonSoFar:]\r\n",
        "    \r\n",
        "    solidCanonSyllables = syllablesFromText(solidCanon)  \r\n",
        "    \r\n",
        "    bestTrial = canonSoFar\r\n",
        "    bestDist = float(\"inf\")\r\n",
        "\r\n",
        "    continuations = Continuations(alts, canonSoFar, i, temperature,\r\n",
        "                                  top_p)\r\n",
        "    for trial in continuations:\r\n",
        "      newText = tentativeCanon + trial[len(canonSoFar):]\r\n",
        "      if (isascii(newText)  # only test generated text that our pronounciation lookups can handle\r\n",
        "          and not any(c.isdigit() for c in newText)):  # discard text with numbers \r\n",
        "        trialFit = canonFit(solidCanonSyllables, newText, offset)\r\n",
        "        if trialFit < bestDist and trialFit > 0: # discourage exact loops\r\n",
        "          bestDist = trialFit\r\n",
        "          bestTrial = trial\r\n",
        "    \r\n",
        "    canonSoFar = bestTrial\r\n",
        "    print()\r\n",
        "    print(canonSoFar)\r\n",
        "\r\n",
        "  print()\r\n",
        "  print(\"FINAL CANON:\")\r\n",
        "  print(canonSoFar)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMJq4r23Iolk",
        "outputId": "17b13e61-c780-425e-b08d-ad840dc8f0eb"
      },
      "source": [
        "generateCanon(alts = 250, tokensPerIncrement=2, length=200, prompt=\"A transition matrix\", offset = 10, temperature = .5, top_p = 0.9)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A transition matrix\n",
            "Generating 250 continuations\n",
            "\n",
            "A transition matrix\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 15.90 GiB total capacity; 14.70 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Generating 122 continuations\n",
            "\n",
            "A transition matrix\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 15.90 GiB total capacity; 14.70 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Generating 122 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 336.00 MiB (GPU 0; 15.90 GiB total capacity; 14.57 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Generating 64 continuations\n",
            "Generating 64 continuations\n",
            "Generating 58 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 15.90 GiB total capacity; 14.61 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 0; 15.90 GiB total capacity; 14.53 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Generating 64 continuations\n",
            "Generating 64 continuations\n",
            "Generating 58 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 528.00 MiB (GPU 0; 15.90 GiB total capacity; 14.65 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 270.00 MiB (GPU 0; 15.90 GiB total capacity; 14.55 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Generating 64 continuations\n",
            "Generating 64 continuations\n",
            "Generating 58 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 320.00 MiB (GPU 0; 15.90 GiB total capacity; 14.57 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 26 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 370.00 MiB (GPU 0; 15.90 GiB total capacity; 14.59 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 186.00 MiB (GPU 0; 15.90 GiB total capacity; 14.52 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 26 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 15.90 GiB total capacity; 14.70 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 418.00 MiB (GPU 0; 15.90 GiB total capacity; 14.60 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 210.00 MiB (GPU 0; 15.90 GiB total capacity; 14.53 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 26 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 15.90 GiB total capacity; 14.71 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 468.00 MiB (GPU 0; 15.90 GiB total capacity; 14.62 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 234.00 MiB (GPU 0; 15.90 GiB total capacity; 14.54 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 26 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 15.90 GiB total capacity; 14.73 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 516.00 MiB (GPU 0; 15.90 GiB total capacity; 14.64 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 0; 15.90 GiB total capacity; 14.54 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 26 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 566.00 MiB (GPU 0; 15.90 GiB total capacity; 14.66 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 284.00 MiB (GPU 0; 15.90 GiB total capacity; 14.55 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 32 continuations\n",
            "Generating 26 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 15.90 GiB total capacity; 14.72 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 15.90 GiB total capacity; 14.75 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 308.00 MiB (GPU 0; 15.90 GiB total capacity; 14.56 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 154.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 15.90 GiB total capacity; 14.66 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 332.00 MiB (GPU 0; 15.90 GiB total capacity; 14.57 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 15.90 GiB total capacity; 14.68 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 356.00 MiB (GPU 0; 15.90 GiB total capacity; 14.58 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 178.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 15.90 GiB total capacity; 14.69 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 15.90 GiB total capacity; 14.80 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 382.00 MiB (GPU 0; 15.90 GiB total capacity; 14.59 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 15.90 GiB total capacity; 14.52 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform,\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 15.90 GiB total capacity; 14.61 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 15.90 GiB total capacity; 14.70 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 406.00 MiB (GPU 0; 15.90 GiB total capacity; 14.60 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 204.00 MiB (GPU 0; 15.90 GiB total capacity; 14.52 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 0; 15.90 GiB total capacity; 14.62 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 0; 15.90 GiB total capacity; 14.77 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 430.00 MiB (GPU 0; 15.90 GiB total capacity; 14.61 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 15.90 GiB total capacity; 14.53 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 0; 15.90 GiB total capacity; 14.63 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 15.90 GiB total capacity; 14.71 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 454.00 MiB (GPU 0; 15.90 GiB total capacity; 14.62 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 15.90 GiB total capacity; 14.53 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 0; 15.90 GiB total capacity; 14.65 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 15.90 GiB total capacity; 14.72 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 480.00 MiB (GPU 0; 15.90 GiB total capacity; 14.63 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 0; 15.90 GiB total capacity; 14.54 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 0; 15.90 GiB total capacity; 14.65 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 15.90 GiB total capacity; 14.73 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 504.00 MiB (GPU 0; 15.90 GiB total capacity; 14.64 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 252.00 MiB (GPU 0; 15.90 GiB total capacity; 14.54 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 0; 15.90 GiB total capacity; 14.66 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 15.90 GiB total capacity; 14.75 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 528.00 MiB (GPU 0; 15.90 GiB total capacity; 14.65 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 264.00 MiB (GPU 0; 15.90 GiB total capacity; 14.55 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 554.00 MiB (GPU 0; 15.90 GiB total capacity; 14.66 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 15.90 GiB total capacity; 14.55 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 16 continuations\n",
            "Generating 10 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 15.90 GiB total capacity; 14.78 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 15.90 GiB total capacity; 14.78 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 15.90 GiB total capacity; 14.79 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 290.00 MiB (GPU 0; 15.90 GiB total capacity; 14.56 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 146.00 MiB (GPU 0; 15.90 GiB total capacity; 14.50 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The fundamental electricity\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 15.90 GiB total capacity; 14.72 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 15.90 GiB total capacity; 14.74 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 15.90 GiB total capacity; 14.56 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 152.00 MiB (GPU 0; 15.90 GiB total capacity; 14.50 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The fundamental electricity of me\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 15.90 GiB total capacity; 14.74 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 15.90 GiB total capacity; 14.65 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 15.90 GiB total capacity; 14.76 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 314.00 MiB (GPU 0; 15.90 GiB total capacity; 14.57 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 158.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The fundamental electricity of me is being\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 15.90 GiB total capacity; 14.68 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 15.90 GiB total capacity; 14.66 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 15.90 GiB total capacity; 14.75 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 15.90 GiB total capacity; 14.57 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The fundamental electricity of me is being pulled apart\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 15.90 GiB total capacity; 14.68 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 15.90 GiB total capacity; 14.67 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 15.90 GiB total capacity; 14.78 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 338.00 MiB (GPU 0; 15.90 GiB total capacity; 14.58 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The fundamental electricity of me is being pulled apart, and\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 15.90 GiB total capacity; 14.69 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 15.90 GiB total capacity; 14.68 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 15.90 GiB total capacity; 14.77 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 350.00 MiB (GPU 0; 15.90 GiB total capacity; 14.58 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 176.00 MiB (GPU 0; 15.90 GiB total capacity; 14.51 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The fundamental electricity of me is being pulled apart, and it is\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 0; 15.90 GiB total capacity; 14.70 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 15.90 GiB total capacity; 14.69 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 15.90 GiB total capacity; 14.78 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 362.00 MiB (GPU 0; 15.90 GiB total capacity; 14.58 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 0; 15.90 GiB total capacity; 14.52 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The fundamental electricity of me is being pulled apart, and it is not being\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 15.90 GiB total capacity; 14.71 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 15.90 GiB total capacity; 14.69 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 15.90 GiB total capacity; 14.77 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 15.90 GiB total capacity; 14.59 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 0; 15.90 GiB total capacity; 14.52 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "\n",
            "A transition matrix is in use.\n",
            "I'm a tree-trunk and when I transform, I'm a fireplace in a tower.\n",
            "When I transform, I can see the patterns of the fire I'm in.\n",
            "The fundamental electricity of me is being pulled apart, and it is not being\n",
            "Generating 250 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 15.90 GiB total capacity; 14.71 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 128 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 15.90 GiB total capacity; 14.69 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 64 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 15.90 GiB total capacity; 14.77 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 32 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 15.90 GiB total capacity; 14.59 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 16 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 0; 15.90 GiB total capacity; 14.52 GiB already allocated; 25.88 MiB free; 14.91 GiB reserved in total by PyTorch)\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 8 continuations\n",
            "Generating 2 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.90 GiB total capacity; 14.47 GiB already allocated; 1.88 MiB free; 14.94 GiB reserved in total by PyTorch)\n",
            "Generating 2 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.90 GiB total capacity; 14.47 GiB already allocated; 1.88 MiB free; 14.94 GiB reserved in total by PyTorch)\n",
            "Generating 2 continuations\n",
            "Runtime Error, reducing batch size: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.90 GiB total capacity; 14.47 GiB already allocated; 1.88 MiB free; 14.94 GiB reserved in total by PyTorch)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXmPunR8-E2w"
      },
      "source": [
        "with new syllabification, temperature 5, top_p .2, tokensPerIncrement = 5, offset 5, params 4/5/2/3 (vowels/stress/onset/coda)\r\n",
        "\r\n",
        "```\r\n",
        "If only I could be in a meeting coordinating this, explaining things.\r\n",
        "Getting these people to believe I can't deliver on time has never made any difference, even in the event that they're irate and cry when I'm explaining why we didn't, I haven't, I haven't, it never was; it never will, it never will; if I were building this case, building this case, building a case about me and [1000Plateaus] I probably would have had a clearer shot at what people thought I was doing than I was.\r\n",
        "I am as bad as I think myself as I can be when it happens to me - I can't be in an arty tent in an alleyway in an arty prison, hardly eating, hardly paying my tab, staring at strangers ferried here from the slobbery of the north, a dozen screaming asylum-climbers and clinging desperately to their property - I have plenty of time, money\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WH8dvGm2miz"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "If  only I  could be in that  boat.       That boat.\r\n",
        "            If    only  I     could be in that boat.\r\n",
        "\r\n",
        "I    don't know why.       Cousinwog, cousin of  Brassy and\r\n",
        "That boat.         I don't know why.  Cousinwog,     cousin \r\n",
        "\r\n",
        "Red from    Cousinwog  and   Mighty from   Cousinwog  \r\n",
        "of  Brassy     and Red from  Cousinwog    and Mighty from\r\n",
        "\r\n",
        "City  and  Plateaus  aren't  together        anymore.   They're\r\n",
        "Cousinwog  City      and     Plateaus aren't together anymore. \r\n",
        "\r\n",
        "having a breakout and Plateaus says \"What's going    on    here?\"\r\n",
        "      They're having   a breakout    and    Plateaus says \"What's\r\n",
        "\r\n",
        "going on here?\"\r\n",
        "```\r\n",
        "\r\n",
        "(offset of 4, temperature .7)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw7ljx8z7Gid"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "If only I  could be my good   editor     in one piece \r\n",
        "        If only     I  could  be my good editor\r\n",
        "\r\n",
        "-- which, at least,   it   was, when  absolutely forced and wri-\r\n",
        "editor   --  which at least,    it   was, when absolutely\r\n",
        "\r\n",
        "ting   ridiculously    long chunks of crap chunks.\r\n",
        "forced and writing ridiculously       long chunks of crap\r\n",
        "\r\n",
        "Without the   ethics.          \"It sucks.\"    Sometimes,\r\n",
        "chunks.       Without  the  ethics.       \"It sucks.\"\r\n",
        "\r\n",
        "sometimes, it's just \"it's difficult       to tell a\r\n",
        "Sometimes, sometimes,      it's just \"it's difficult\r\n",
        "\r\n",
        "story\"   as art,   or an illusion.    There's another,     ever\r\n",
        "to tell  a  story\" as art, or an illusion.       There's another,\r\n",
        "\r\n",
        "another trance.  Sometimes it's \"I don't  think it's\r\n",
        "   ever another  trance.        Sometimes it's  \"I\r\n",
        "\r\n",
        "important\"         as   entertainment.\r\n",
        "don't think it's   important\" as  entertainment.\r\n",
        "```\r\n",
        "(offset of 3, temperature 2.0, vowels 4, stress 3, consonants 5)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGcVE1FaA4hj"
      },
      "source": [
        "```\r\n",
        "If only I could be allowed to  be loved.     Been  avoiding\r\n",
        "          If    only  I  could be allowed to be    loved.\r\n",
        "\r\n",
        "eye  doctor since  ninety-two.\r\n",
        "Been avoiding      eye doctor  since ninety-two.\r\n",
        "```\r\n",
        "\r\n",
        "(offset of 4, temperature 2, vowels, stress, cons = 2,1,2; 3 tokens per increment)\r\n",
        "\r\n",
        "```\r\n",
        "If only I could be that good.\r\n",
        "People say it - \"It's true it isn't\" - \"It is\", \"It is true it is true\" - \"Dear @lukejchengnott, in what way?\"\r\n",
        "```\r\n",
        "\r\n",
        "(same settings, but offset of 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAhT-dl3H8oB"
      },
      "source": [
        "```\r\n",
        "If only\r\n",
        "I could be\r\n",
        "as good as\r\n",
        "this.\r\n",
        "I know I'm\r\n",
        "doing ev\r\n",
        "ery thing right.\r\n",
        "          I\r\n",
        "know I'm do\r\n",
        "ing every\r\n",
        "thing I can.\r\n",
        "          I\r\n",
        "know I'm do\r\n",
        "ing every\r\n",
        "thing I can.\r\n",
        "          I\r\n",
        "know I'm cap\r\n",
        "able.\r\n",
        "     I be\r\n",
        "lieve I am\r\n",
        "loved.\r\n",
        "I believe\r\n",
        "I am ent\r\n",
        "ertaining\r\n",
        "the impos\r\n",
        "sible.\r\n",
        "  But all\r\n",
        "of a   sud\r\n",
        "den it seems\r\n",
        "like it's a\r\n",
        "bout to col\r\n",
        "lapse on it\r\n",
        "self and I\r\n",
        "have to find\r\n",
        "a place to\r\n",
        "stand,  so\r\n",
        "I don't lis\r\n",
        "ten.\r\n",
        "Finally\r\n",
        "I find a\r\n",
        "place to sit,\r\n",
        "   thinking\r\n",
        "it's okay\r\n",
        "to be there.\r\n",
        "      There\r\n",
        "is a rea\r\n",
        "sonable\r\n",
        "place to go,\r\n",
        "    but I'm\r\n",
        "worried it\r\n",
        "will be rai\r\n",
        "ded by the\r\n",
        "big demon\r\n",
        "ic fuckups,\r\n",
        "   and killed.\r\n",
        "          I\r\n",
        "know that I\r\n",
        "am allowed\r\n",
        "to feel my\r\n",
        "anger and\r\n",
        "anxie\r\n",
        "ty under\r\n",
        "control,\r\n",
        "that this is\r\n",
        "an extreme\r\n",
        "ly danger\r\n",
        "ous place.\r\n",
        "    I sit\r\n",
        "on it,\r\n",
        "thinking it's\r\n",
        "really o\r\n",
        "kay to be\r\n",
        "there.\r\n",
        "But then I\r\n",
        "get really\r\n",
        "worried and\r\n",
        "angry   and\r\n",
        "a     pile  of\r\n",
        "Trash piles up.\r\n",
        "```\r\n",
        "\r\n",
        "(alts -> 50 from 250, 5 tokens per increment, temperature 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77lE-sKzKLGt"
      },
      "source": [
        "(stress weight 1->200, offset -> 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ohIoeqXahkw",
        "outputId": "a72fc1ab-6b39-47f5-c87d-efac20e8d2c1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 363
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOWldxrsqVjh"
      },
      "source": [
        "kDistanceWeightVowels = 3.0\r\n",
        "kDistanceWeightStress = 3.0\r\n",
        "kDistanceWeightOnset = 2.0\r\n",
        "kDistanceWeightCoda = 5.0\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iulVyEdBa-Wr",
        "outputId": "eb0ac510-0ef5-4c65-e4b5-c62e9ea5b422"
      },
      "source": [
        "syllabify.syllabify(phones_for_word_fb(\"transmute\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['T', 'R'], ['AE0'], ['N', 'S']), (['M'], ['Y', 'UW1'], ['T'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2VxdaZ8bHZC"
      },
      "source": [
        "kDistanceWeightVowels = 3.0\r\n",
        "kDistanceWeightStress = 3.0\r\n",
        "kDistanceWeightOnset = 2.0\r\n",
        "kDistanceWeightCoda = 5.0\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBy6nfHgv7rt",
        "outputId": "d4e70ac7-711a-4e14-f3ed-f3ed3440c015"
      },
      "source": [
        "kDistanceWeightCoda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}